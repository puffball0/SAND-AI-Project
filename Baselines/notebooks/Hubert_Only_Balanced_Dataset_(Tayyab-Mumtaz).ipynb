{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147130b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ HUBERT ONLY FOR SAND CHALLENGE\n",
      "Strategy: HuBERT Features Only + Balanced Dataset\n",
      "Target: F1 Score >= 0.70 | High Accuracy\n",
      "================================================================================\n",
      "âœ… Using device: cpu\n",
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ HUBERT ONLY FOR SAND CHALLENGE\")\n",
    "print(\"Strategy: HuBERT Features Only + Balanced Dataset\")\n",
    "print(\"Target: F1 Score >= 0.70 | High Accuracy\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, cohen_kappa_score\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56689c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“‹ CONFIGURATION - HUBERT ONLY + BALANCED DATASET\n",
      "================================================================================\n",
      "HuBERT Model: facebook/hubert-base-ls960\n",
      "Feature Strategy: PURE HUBERT FEATURES ONLY\n",
      "Statistical Features: False (DISABLED)\n",
      "Dataset Strategy: BALANCED (SMOTE)\n",
      "Classifier: SVM (C=100.0)\n",
      "Multi-Layer Extraction: True (4 layers)\n",
      "Multi-Pooling: True (4 strategies)\n",
      "PCA: True (variance=0.95)\n",
      "SMOTE: True (strategy=auto)\n",
      "Device: cpu\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Verifying paths...\n",
      "âœ… All paths verified!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: CONFIGURATION FOR HUBERT ONLY + BALANCED DATASET\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    \"\"\"Configuration for HuBERT Features Only with Balanced Dataset\"\"\"\n",
    "\n",
    "    # Paths - UPDATE THESE PATHS FOR YOUR LOCAL SYSTEM\n",
    "    BASE_PATH = 'SAND_Project_Data'  # Update this path\n",
    "    TRAINING_PATH = os.path.join(BASE_PATH, 'training')\n",
    "    EXCEL_PATH = os.path.join(BASE_PATH, 'sand_task_1.xlsx')\n",
    "    OUTPUT_PATH = os.path.join(BASE_PATH, 'hubert_only_balanced')\n",
    "\n",
    "    # Sheet names\n",
    "    SHEET_TRAINING = 'Training Baseline - Task 1'\n",
    "    SHEET_VALIDATION = 'Validation Baseline - Task 1'\n",
    "\n",
    "    # Audio types\n",
    "    AUDIO_TYPES = ['phonationA', 'phonationE', 'phonationI', 'phonationO', 'phonationU',\n",
    "                   'rhythmKA', 'rhythmPA', 'rhythmTA']\n",
    "\n",
    "    # ========================================================================\n",
    "    # HUBERT CONFIGURATION - OPTIMIZED FOR PURE HUBERT FEATURES\n",
    "    # ========================================================================\n",
    "    HUBERT_MODEL = 'facebook/hubert-base-ls960'\n",
    "    SAMPLING_RATE = 16000\n",
    "    MAX_AUDIO_LENGTH = 16000 * 15  # 15 seconds\n",
    "\n",
    "    # Multi-layer feature extraction (FOCUS ON HUBERT ONLY)\n",
    "    USE_MULTI_LAYER = True\n",
    "    LAYERS_TO_USE = [-4, -3, -2, -1]  # Last 4 layers\n",
    "\n",
    "    # Multi-pooling strategy (MAXIMIZE HUBERT INFORMATION)\n",
    "    USE_MULTI_POOLING = True\n",
    "    POOLING_STRATEGIES = ['mean', 'std', 'max', 'min']  # 4 pooling methods\n",
    "\n",
    "    # Statistical features - DISABLED FOR PURE HUBERT APPROACH\n",
    "    USE_STATISTICAL_FEATURES = False  # Pure HuBERT features only\n",
    "\n",
    "    # Audio augmentation during loading\n",
    "    AUDIO_TRIM_DB = 15  # Lower threshold for dysarthric speech\n",
    "    AUDIO_NORMALIZE = True\n",
    "    MIN_AUDIO_LENGTH_SEC = 2\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREPROCESSING OPTIMIZATIONS\n",
    "    # ========================================================================\n",
    "    USE_ROBUST_SCALING = True  # Better for outliers\n",
    "\n",
    "    # PCA Configuration - MORE AGGRESSIVE FOR HUBERT FEATURES\n",
    "    USE_PCA = True\n",
    "    PCA_VARIANCE = 0.95  # Slightly lower to focus on most important HuBERT features\n",
    "\n",
    "    # ========================================================================\n",
    "    # DATA BALANCING STRATEGIES - FOCUS ON BALANCED DATASET\n",
    "    # ========================================================================\n",
    "    USE_SMOTE = True\n",
    "    SMOTE_STRATEGY = 'auto'  # Balance all classes equally\n",
    "    SMOTE_K_NEIGHBORS = 3\n",
    "\n",
    "    # ========================================================================\n",
    "    # CLASSIFIER CONFIGURATION - SVM FOR HUBERT FEATURES\n",
    "    # ========================================================================\n",
    "    SVM_CONFIG = {\n",
    "        'kernel': 'rbf',\n",
    "        'C': 100.0,  # Optimized for HuBERT features\n",
    "        'gamma': 'scale',\n",
    "        'class_weight': 'balanced',\n",
    "        'probability': True,\n",
    "        'decision_function_shape': 'ovr',\n",
    "        'max_iter': 10000,\n",
    "        'cache_size': 2000,\n",
    "        'tol': 1e-4\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    N_FOLDS = 5\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # Class names\n",
    "    CLASS_NAMES = {\n",
    "        0: 'Severe Dysarthria',\n",
    "        1: 'Moderate Dysarthria',\n",
    "        2: 'Mild Dysarthria',\n",
    "        3: 'No Dysarthria (ALS)',\n",
    "        4: 'Healthy'\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ CONFIGURATION - HUBERT ONLY + BALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"HuBERT Model: {config.HUBERT_MODEL}\")\n",
    "print(f\"Feature Strategy: PURE HUBERT FEATURES ONLY\")\n",
    "print(f\"Statistical Features: {config.USE_STATISTICAL_FEATURES} (DISABLED)\")\n",
    "print(f\"Dataset Strategy: BALANCED (SMOTE)\")\n",
    "print(f\"Classifier: SVM (C={config.SVM_CONFIG['C']})\")\n",
    "print(f\"Multi-Layer Extraction: {config.USE_MULTI_LAYER} ({len(config.LAYERS_TO_USE)} layers)\")\n",
    "print(f\"Multi-Pooling: {config.USE_MULTI_POOLING} ({len(config.POOLING_STRATEGIES)} strategies)\")\n",
    "print(f\"PCA: {config.USE_PCA} (variance={config.PCA_VARIANCE})\")\n",
    "print(f\"SMOTE: {config.USE_SMOTE} (strategy={config.SMOTE_STRATEGY})\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"\\nðŸ” Verifying paths...\")\n",
    "assert os.path.exists(config.BASE_PATH), f\"Base path not found: {config.BASE_PATH}\"\n",
    "assert os.path.exists(config.TRAINING_PATH), f\"Training path not found: {config.TRAINING_PATH}\"\n",
    "assert os.path.exists(config.EXCEL_PATH), f\"Excel file not found: {config.EXCEL_PATH}\"\n",
    "print(\"âœ… All paths verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0057cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1/9] LOADING DATASETS\n",
      "================================================================================\n",
      "\n",
      "âœ… Loaded 219 samples from: 'Training Baseline - Task 1'\n",
      "ðŸ“Š Class distribution:\n",
      "  Class 0 (Severe Dysarthria        ):   4 ( 1.83%)\n",
      "  Class 1 (Moderate Dysarthria      ):  22 (10.05%)\n",
      "  Class 2 (Mild Dysarthria          ):  45 (20.55%)\n",
      "  Class 3 (No Dysarthria (ALS)      ):  62 (28.31%)\n",
      "  Class 4 (Healthy                  ):  86 (39.27%)\n",
      "\n",
      "âœ… Loaded 53 samples from: 'Validation Baseline - Task 1'\n",
      "ðŸ“Š Class distribution:\n",
      "  Class 0 (Severe Dysarthria        ):   2 ( 3.77%)\n",
      "  Class 1 (Moderate Dysarthria      ):   4 ( 7.55%)\n",
      "  Class 2 (Mild Dysarthria          ):  12 (22.64%)\n",
      "  Class 3 (No Dysarthria (ALS)      ):  14 (26.42%)\n",
      "  Class 4 (Healthy                  ):  21 (39.62%)\n",
      "\n",
      "ðŸ“ˆ Summary:\n",
      "  Training samples: 219\n",
      "  Validation samples: 53\n",
      "  Total audio files: 2176\n",
      "  Overlap check: âœ… No overlap\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: LOAD DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[1/9] LOADING DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_data(excel_path, sheet_name):\n",
    "    \"\"\"Load dataset from Excel\"\"\"\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    print(f\"\\nâœ… Loaded {len(df)} samples from: '{sheet_name}'\")\n",
    "\n",
    "    # Convert to 0-indexed classes\n",
    "    df['Class'] = df['Class'] - 1\n",
    "\n",
    "    # Display distribution\n",
    "    print(f\"ðŸ“Š Class distribution:\")\n",
    "    class_dist = df['Class'].value_counts().sort_index()\n",
    "    for cls, count in class_dist.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  Class {cls} ({config.CLASS_NAMES[cls]:25s}): {count:3d} ({pct:5.2f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load training and validation sets\n",
    "df_train = load_data(config.EXCEL_PATH, config.SHEET_TRAINING)\n",
    "df_val = load_data(config.EXCEL_PATH, config.SHEET_VALIDATION)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Summary:\")\n",
    "print(f\"  Training samples: {len(df_train)}\")\n",
    "print(f\"  Validation samples: {len(df_val)}\")\n",
    "print(f\"  Total audio files: {(len(df_train) + len(df_val)) * len(config.AUDIO_TYPES)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_ids = set(df_train['ID'].values)\n",
    "val_ids = set(df_val['ID'].values)\n",
    "overlap = train_ids.intersection(val_ids)\n",
    "print(f\"  Overlap check: {'âœ… No overlap' if len(overlap) == 0 else f'âš  {len(overlap)} overlapping IDs'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49751df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2/9] LOADING HUBERT MODEL\n",
      "================================================================================\n",
      "Model: facebook/hubert-base-ls960\n",
      "â± This may take 1-2 minutes for first-time download...\n",
      "\n",
      "âœ… HuBERT loaded successfully!\n",
      "  Model size: 94.4M parameters\n",
      "  Hidden size: 768\n",
      "  Number of layers: 12\n",
      "  Sampling rate: 16000 Hz\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: LOAD HUBERT MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2/9] LOADING HUBERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {config.HUBERT_MODEL}\")\n",
    "print(\"â± This may take 1-2 minutes for first-time download...\")\n",
    "\n",
    "# Load feature extractor and model\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.HUBERT_MODEL)\n",
    "hubert_model = HubertModel.from_pretrained(config.HUBERT_MODEL)\n",
    "hubert_model = hubert_model.to(device)\n",
    "hubert_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"\\nâœ… HuBERT loaded successfully!\")\n",
    "print(f\"  Model size: {sum(p.numel() for p in hubert_model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"  Hidden size: {hubert_model.config.hidden_size}\")\n",
    "print(f\"  Number of layers: {hubert_model.config.num_hidden_layers}\")\n",
    "print(f\"  Sampling rate: {feature_extractor.sampling_rate} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08230f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[3/9] DEFINING HUBERT-ONLY AUDIO PROCESSING\n",
      "================================================================================\n",
      "âœ… HuBERT-only audio processing functions defined!\n",
      "  Enhanced audio loading: âœ“\n",
      "  Pure HuBERT features: âœ“ (NO statistical features)\n",
      "  Multi-layer HuBERT extraction: âœ“\n",
      "  Feature composition: 100% HuBERT representations\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: AUDIO PROCESSING FUNCTIONS - HUBERT ONLY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3/9] DEFINING HUBERT-ONLY AUDIO PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_audio_enhanced(audio_path, target_sr=16000, max_length=None):\n",
    "    \"\"\"\n",
    "    Enhanced audio loading with advanced preprocessing\n",
    "    Focus on optimal input for HuBERT features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path, sr=target_sr, mono=True)\n",
    "\n",
    "        # Advanced trimming with lower threshold for dysarthric speech\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=config.AUDIO_TRIM_DB)\n",
    "\n",
    "        # Normalize amplitude for consistent HuBERT processing\n",
    "        if config.AUDIO_NORMALIZE:\n",
    "            audio = librosa.util.normalize(audio)\n",
    "\n",
    "        # Handle length constraints\n",
    "        if max_length and len(audio) > max_length:\n",
    "            audio = audio[:max_length]\n",
    "\n",
    "        # Ensure minimum length for meaningful HuBERT features\n",
    "        min_length = target_sr * config.MIN_AUDIO_LENGTH_SEC\n",
    "        if len(audio) < min_length:\n",
    "            audio = np.pad(audio, (0, min_length - len(audio)), mode='constant')\n",
    "\n",
    "        return audio, sr\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error loading {audio_path}: {str(e)}\")\n",
    "        # Return silence as fallback\n",
    "        return np.zeros(target_sr * config.MIN_AUDIO_LENGTH_SEC), target_sr\n",
    "\n",
    "def extract_hubert_features_multilayer(audio, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    Extract PURE HuBERT features with multi-layer and multi-pooling strategy\n",
    "    NO statistical features - Pure HuBERT representation only\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess audio for HuBERT\n",
    "        inputs = feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Extract features with all hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = hubert_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        all_features = []\n",
    "\n",
    "        # Extract from multiple layers\n",
    "        if config.USE_MULTI_LAYER:\n",
    "            layers_to_process = config.LAYERS_TO_USE\n",
    "        else:\n",
    "            layers_to_process = [-1]  # Only last layer\n",
    "\n",
    "        for layer_idx in layers_to_process:\n",
    "            hidden_state = outputs.hidden_states[layer_idx]  # Shape: [batch, time, hidden]\n",
    "\n",
    "            # Apply multiple pooling strategies\n",
    "            if config.USE_MULTI_POOLING:\n",
    "                pooling_methods = config.POOLING_STRATEGIES\n",
    "            else:\n",
    "                pooling_methods = ['mean']  # Only mean pooling\n",
    "\n",
    "            for pool_strategy in pooling_methods:\n",
    "                if pool_strategy == 'mean':\n",
    "                    pooled = hidden_state.mean(dim=1)\n",
    "                elif pool_strategy == 'max':\n",
    "                    pooled = hidden_state.max(dim=1)[0]\n",
    "                elif pool_strategy == 'std':\n",
    "                    pooled = hidden_state.std(dim=1)\n",
    "                elif pool_strategy == 'min':\n",
    "                    pooled = hidden_state.min(dim=1)[0]\n",
    "                else:\n",
    "                    pooled = hidden_state.mean(dim=1)\n",
    "\n",
    "                all_features.append(pooled.cpu().numpy().squeeze())\n",
    "\n",
    "        # Concatenate all PURE HuBERT features\n",
    "        hubert_features = np.concatenate(all_features)\n",
    "\n",
    "        return hubert_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  HuBERT feature extraction error: {str(e)}\")\n",
    "        # Return zero features as fallback\n",
    "        n_layers = len(config.LAYERS_TO_USE) if config.USE_MULTI_LAYER else 1\n",
    "        n_pooling = len(config.POOLING_STRATEGIES) if config.USE_MULTI_POOLING else 1\n",
    "        feature_dim = hubert_model.config.hidden_size * n_layers * n_pooling\n",
    "        return np.zeros(feature_dim)\n",
    "\n",
    "print(\"âœ… HuBERT-only audio processing functions defined!\")\n",
    "print(f\"  Enhanced audio loading: âœ“\")\n",
    "print(f\"  Pure HuBERT features: âœ“ (NO statistical features)\")\n",
    "print(f\"  Multi-layer HuBERT extraction: âœ“\")\n",
    "print(f\"  Feature composition: 100% HuBERT representations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71aa349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[4/9] EXTRACTING PURE HUBERT FEATURES\n",
      "================================================================================\n",
      "Configuration:\n",
      "  Layers: 4\n",
      "  Pooling strategies: 4\n",
      "  Statistical features: False (DISABLED)\n",
      "  Feature multiplication factor: 16x\n",
      "  Pure HuBERT feature dimension: 12288\n",
      "\n",
      "ðŸŽµ Training 219 patients with PURE HUBERT features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b502a855b294e9b934c63c605e63e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽµ Validation 53 patients with PURE HUBERT features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17621ce46904ea38c17e89e586f3dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Pure HuBERT feature extraction complete!\n",
      "  Training shape: (219, 98304)\n",
      "  Validation shape: (53, 98304)\n",
      "  Feature dimension per patient: 98304\n",
      "  Feature composition: 100% HuBERT representations\n",
      "  Total HuBERT features: 98304\n",
      "  Data cleaned: âœ“\n",
      "  ðŸ’¾ Pure HuBERT features saved: SAND_Project_Data\\hubert_only_balanced\\features_hubert_only.npz\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: EXTRACT PURE HUBERT FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4/9] EXTRACTING PURE HUBERT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Layers: {len(config.LAYERS_TO_USE)}\")\n",
    "print(f\"  Pooling strategies: {len(config.POOLING_STRATEGIES)}\")\n",
    "print(f\"  Statistical features: {config.USE_STATISTICAL_FEATURES} (DISABLED)\")\n",
    "print(f\"  Feature multiplication factor: {len(config.LAYERS_TO_USE) * len(config.POOLING_STRATEGIES)}x\")\n",
    "print(f\"  Pure HuBERT feature dimension: {hubert_model.config.hidden_size * len(config.LAYERS_TO_USE) * len(config.POOLING_STRATEGIES)}\")\n",
    "\n",
    "def extract_features_for_dataset(df, dataset_path, audio_types, desc=\"Processing\"):\n",
    "    \"\"\"\n",
    "    Extract PURE HuBERT features for entire dataset\n",
    "    No statistical features - only HuBERT representations\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    ids_list = []\n",
    "\n",
    "    print(f\"\\nðŸŽµ {desc} {len(df)} patients with PURE HUBERT features...\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        patient_id = row['ID']\n",
    "        patient_class = row['Class']\n",
    "\n",
    "        patient_features = []\n",
    "\n",
    "        # Process each audio type\n",
    "        for audio_type in audio_types:\n",
    "            audio_file = f\"{patient_id}_{audio_type}.wav\"\n",
    "            audio_path = os.path.join(dataset_path, audio_type, audio_file)\n",
    "\n",
    "            if os.path.exists(audio_path):\n",
    "                # Load audio with enhancements\n",
    "                audio, sr = load_audio_enhanced(\n",
    "                    audio_path,\n",
    "                    target_sr=config.SAMPLING_RATE,\n",
    "                    max_length=config.MAX_AUDIO_LENGTH\n",
    "                )\n",
    "\n",
    "                # Extract PURE HuBERT features only\n",
    "                hubert_features = extract_hubert_features_multilayer(audio, sr)\n",
    "\n",
    "                # Use only HuBERT features (no statistical features)\n",
    "                combined_features = hubert_features\n",
    "\n",
    "                patient_features.append(combined_features)\n",
    "            else:\n",
    "                # Zero features for missing files (HuBERT-only dimension)\n",
    "                n_layers = len(config.LAYERS_TO_USE) if config.USE_MULTI_LAYER else 1\n",
    "                n_pooling = len(config.POOLING_STRATEGIES) if config.USE_MULTI_POOLING else 1\n",
    "                feature_dim = hubert_model.config.hidden_size * n_layers * n_pooling\n",
    "\n",
    "                patient_features.append(np.zeros(feature_dim))\n",
    "\n",
    "        # Concatenate features from all audio types\n",
    "        combined = np.concatenate(patient_features)\n",
    "        features_list.append(combined)\n",
    "        labels_list.append(patient_class)\n",
    "        ids_list.append(patient_id)\n",
    "\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(labels_list)\n",
    "    ids = np.array(ids_list)\n",
    "\n",
    "    return X, y, ids\n",
    "\n",
    "# Extract training features\n",
    "X_train, y_train, ids_train = extract_features_for_dataset(\n",
    "    df_train,\n",
    "    config.TRAINING_PATH,\n",
    "    config.AUDIO_TYPES,\n",
    "    desc=\"Training\"\n",
    ")\n",
    "\n",
    "# Extract validation features\n",
    "X_val, y_val, ids_val = extract_features_for_dataset(\n",
    "    df_val,\n",
    "    config.TRAINING_PATH,\n",
    "    config.AUDIO_TYPES,\n",
    "    desc=\"Validation\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Pure HuBERT feature extraction complete!\")\n",
    "print(f\"  Training shape: {X_train.shape}\")\n",
    "print(f\"  Validation shape: {X_val.shape}\")\n",
    "print(f\"  Feature dimension per patient: {X_train.shape[1]}\")\n",
    "print(f\"  Feature composition: 100% HuBERT representations\")\n",
    "print(f\"  Total HuBERT features: {X_train.shape[1]}\")\n",
    "\n",
    "# Clean data (handle any NaN/Inf)\n",
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"  Data cleaned: âœ“\")\n",
    "\n",
    "# Save raw features\n",
    "features_path = os.path.join(config.OUTPUT_PATH, 'features_hubert_only.npz')\n",
    "np.savez(\n",
    "    features_path,\n",
    "    X_train=X_train, y_train=y_train, ids_train=ids_train,\n",
    "    X_val=X_val, y_val=y_val, ids_val=ids_val\n",
    ")\n",
    "print(f\"  ðŸ’¾ Pure HuBERT features saved: {features_path}\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759b8e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[5/9] PREPROCESSING FOR BALANCED DATASET\n",
      "================================================================================\n",
      "ðŸ“Š Applying RobustScaler (better for HuBERT feature outliers)...\n",
      "  âœ… Scaling complete\n",
      "     Train: mean=0.021912, std=0.779400\n",
      "     Val:   mean=0.033591, std=0.818913\n",
      "\n",
      "ðŸ“‰ Applying PCA (variance threshold=0.95)...\n",
      "  âœ… PCA complete\n",
      "     Original HuBERT dimensions: 98304\n",
      "     Reduced dimensions: 187\n",
      "     Explained variance: 95.01%\n",
      "     Dimensionality reduction: 525.7x\n",
      "     Top 5 PCA components explain: 22.99% variance\n",
      "\n",
      "âš–ï¸ Applying SMOTE for balanced dataset (strategy='auto')...\n",
      "  Original class distribution:\n",
      "    Class 0 (Severe Dysarthria        ):   4 (  1.8%)\n",
      "    Class 1 (Moderate Dysarthria      ):  22 ( 10.0%)\n",
      "    Class 2 (Mild Dysarthria          ):  45 ( 20.5%)\n",
      "    Class 3 (No Dysarthria (ALS)      ):  62 ( 28.3%)\n",
      "    Class 4 (Healthy                  ):  86 ( 39.3%)\n",
      "\n",
      "  âœ… SMOTE balancing complete\n",
      "     Samples: 219 â†’ 430\n",
      "  Balanced class distribution:\n",
      "    Class 0 (Severe Dysarthria        ):  86 ( 20.0%)\n",
      "    Class 1 (Moderate Dysarthria      ):  86 ( 20.0%)\n",
      "    Class 2 (Mild Dysarthria          ):  86 ( 20.0%)\n",
      "    Class 3 (No Dysarthria (ALS)      ):  86 ( 20.0%)\n",
      "    Class 4 (Healthy                  ):  86 ( 20.0%)\n",
      "\n",
      "âœ… Preprocessing complete! Using balanced dataset with pure HuBERT features.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: PREPROCESSING PIPELINE - BALANCED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5/9] PREPROCESSING FOR BALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Scaling\n",
    "if config.USE_ROBUST_SCALING:\n",
    "    print(\"ðŸ“Š Applying RobustScaler (better for HuBERT feature outliers)...\")\n",
    "    scaler = RobustScaler()\n",
    "else:\n",
    "    print(\"ðŸ“Š Applying StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"  âœ… Scaling complete\")\n",
    "print(f\"     Train: mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")\n",
    "print(f\"     Val:   mean={X_val_scaled.mean():.6f}, std={X_val_scaled.std():.6f}\")\n",
    "\n",
    "# Step 2: PCA (Dimensionality Reduction for HuBERT features)\n",
    "if config.USE_PCA:\n",
    "    print(f\"\\nðŸ“‰ Applying PCA (variance threshold={config.PCA_VARIANCE})...\")\n",
    "    pca = PCA(n_components=config.PCA_VARIANCE, random_state=config.RANDOM_SEED)\n",
    "    X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "    X_val_scaled = pca.transform(X_val_scaled)\n",
    "\n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"  âœ… PCA complete\")\n",
    "    print(f\"     Original HuBERT dimensions: {X_train.shape[1]}\")\n",
    "    print(f\"     Reduced dimensions: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"     Explained variance: {explained_var*100:.2f}%\")\n",
    "    print(f\"     Dimensionality reduction: {X_train.shape[1]/X_train_scaled.shape[1]:.1f}x\")\n",
    "    \n",
    "    # Analyze PCA components\n",
    "    print(f\"     Top 5 PCA components explain: {pca.explained_variance_ratio_[:5].sum()*100:.2f}% variance\")\n",
    "else:\n",
    "    pca = None\n",
    "    print(\"  â„¹ PCA disabled\")\n",
    "\n",
    "# Step 3: Data Balancing with SMOTE\n",
    "if config.USE_SMOTE:\n",
    "    print(f\"\\nâš–ï¸ Applying SMOTE for balanced dataset (strategy='{config.SMOTE_STRATEGY}')...\")\n",
    "    print(f\"  Original class distribution:\")\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    total_samples = len(y_train)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        pct = (count / total_samples) * 100\n",
    "        print(f\"    Class {cls} ({config.CLASS_NAMES[cls]:25s}): {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "    try:\n",
    "        smote = SMOTE(\n",
    "            sampling_strategy=config.SMOTE_STRATEGY,\n",
    "            k_neighbors=config.SMOTE_K_NEIGHBORS,\n",
    "            random_state=config.RANDOM_SEED\n",
    "        )\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "        print(f\"\\n  âœ… SMOTE balancing complete\")\n",
    "        print(f\"     Samples: {len(y_train)} â†’ {len(y_train_resampled)}\")\n",
    "        print(f\"  Balanced class distribution:\")\n",
    "        unique, counts = np.unique(y_train_resampled, return_counts=True)\n",
    "        for cls, count in zip(unique, counts):\n",
    "            pct = (count / len(y_train_resampled)) * 100\n",
    "            print(f\"    Class {cls} ({config.CLASS_NAMES[cls]:25s}): {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âš  SMOTE failed: {e}\")\n",
    "        print(f\"  Continuing without balancing...\")\n",
    "        X_train_resampled = X_train_scaled\n",
    "        y_train_resampled = y_train\n",
    "else:\n",
    "    X_train_resampled = X_train_scaled\n",
    "    y_train_resampled = y_train\n",
    "    print(\"  â„¹ SMOTE disabled - using original unbalanced data\")\n",
    "\n",
    "print(\"\\nâœ… Preprocessing complete! Using balanced dataset with pure HuBERT features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a07b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[6/9] TRAINING SVM ON PURE HUBERT FEATURES\n",
      "================================================================================\n",
      "Configuration:\n",
      "  kernel: rbf\n",
      "  C: 100.0\n",
      "  gamma: scale\n",
      "  class_weight: balanced\n",
      "  probability: True\n",
      "  decision_function_shape: ovr\n",
      "  max_iter: 10000\n",
      "\n",
      "ðŸŽ¯ Training SVM on 430 balanced samples...\n",
      "   Features: 100% Pure HuBERT representations\n",
      "   Dataset: BALANCED\n",
      "   Feature dimension: 187\n",
      "âœ… SVM training complete!\n",
      "  Number of support vectors: 275\n",
      "  Support vectors per class: [12 48 55 74 86]\n",
      "  Feature space: Pure HuBERT representations\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: TRAIN SVM CLASSIFIER ON HUBERT FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6/9] TRAINING SVM ON PURE HUBERT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration:\")\n",
    "for key, value in config.SVM_CONFIG.items():\n",
    "    if key not in ['cache_size', 'tol']:  # Skip technical parameters\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Training SVM on {len(y_train_resampled)} balanced samples...\")\n",
    "print(f\"   Features: 100% Pure HuBERT representations\")\n",
    "print(f\"   Dataset: {'BALANCED' if config.USE_SMOTE else 'UNBALANCED'}\")\n",
    "print(f\"   Feature dimension: {X_train_resampled.shape[1]}\")\n",
    "\n",
    "# Initialize and train classifier\n",
    "classifier = SVC(**config.SVM_CONFIG)\n",
    "classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(f\"âœ… SVM training complete!\")\n",
    "print(f\"  Number of support vectors: {classifier.n_support_.sum()}\")\n",
    "print(f\"  Support vectors per class: {classifier.n_support_}\")\n",
    "print(f\"  Feature space: Pure HuBERT representations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd55de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[7/9] CROSS-VALIDATION ON BALANCED DATA\n",
      "================================================================================\n",
      "Performing 5-fold stratified cross-validation...\n",
      "â± This may take a few minutes...\n",
      "\n",
      "âœ… Cross-validation complete!\n",
      "  Fold scores: ['0.7980', '0.8131', '0.7808', '0.8155', '0.8827']\n",
      "  Mean CV F1: 0.8180\n",
      "  Std CV F1:  0.0347\n",
      "  Min CV F1:  0.7808\n",
      "  Max CV F1:  0.8827\n",
      "  Mean CV Accuracy: 0.8256\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: CROSS-VALIDATION ON BALANCED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7/9] CROSS-VALIDATION ON BALANCED DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Performing {config.N_FOLDS}-fold stratified cross-validation...\")\n",
    "print(\"â± This may take a few minutes...\")\n",
    "\n",
    "# Cross-validation on balanced training set\n",
    "cv_scores = cross_val_score(\n",
    "    SVC(**config.SVM_CONFIG),\n",
    "    X_train_resampled,  # Use balanced data for CV\n",
    "    y_train_resampled,\n",
    "    cv=config.N_FOLDS,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Cross-validation complete!\")\n",
    "print(f\"  Fold scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "print(f\"  Mean CV F1: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std CV F1:  {cv_scores.std():.4f}\")\n",
    "print(f\"  Min CV F1:  {cv_scores.min():.4f}\")\n",
    "print(f\"  Max CV F1:  {cv_scores.max():.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "cv_accuracy = cross_val_score(\n",
    "    SVC(**config.SVM_CONFIG),\n",
    "    X_train_resampled,\n",
    "    y_train_resampled,\n",
    "    cv=config.N_FOLDS,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"  Mean CV Accuracy: {cv_accuracy.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf0e912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[8/9] VALIDATION SET EVALUATION - PURE HUBERT FEATURES\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ FINAL RESULTS - PURE HUBERT FEATURES + BALANCED DATASET\n",
      "================================================================================\n",
      "Feature Strategy: 100% HUBERT REPRESENTATIONS\n",
      "Statistical Features: False (DISABLED)\n",
      "Dataset Strategy: BALANCED\n",
      "Classifier: SVM (C=100.0)\n",
      "Validation Accuracy:      0.5849\n",
      "F1 Score (Macro):         0.4718 â­\n",
      "F1 Score (Weighted):      0.5651\n",
      "Precision (Macro):        0.4467\n",
      "Recall (Macro):           0.5381\n",
      "Cohen's Kappa:            0.4276\n",
      "CV F1 (Mean Â± Std):       0.8180 Â± 0.0347\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PER-CLASS PERFORMANCE (PURE HUBERT FEATURES):\n",
      "  Class 0 (Severe Dysarthria        ): 0.0000 \n",
      "  Class 1 (Moderate Dysarthria      ): 0.6667 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Class 2 (Mild Dysarthria          ): 0.5926 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Class 3 (No Dysarthria (ALS)      ): 0.4167 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Class 4 (Healthy                  ): 0.6829 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CLASSIFICATION REPORT:\n",
      "--------------------------------------------------------------------------------\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  Severe Dysarthria       0.00      0.00      0.00         2\n",
      "Moderate Dysarthria       0.50      1.00      0.67         4\n",
      "    Mild Dysarthria       0.53      0.67      0.59        12\n",
      "No Dysarthria (ALS)       0.50      0.36      0.42        14\n",
      "            Healthy       0.70      0.67      0.68        21\n",
      "\n",
      "           accuracy                           0.58        53\n",
      "          macro avg       0.45      0.54      0.47        53\n",
      "       weighted avg       0.57      0.58      0.57        53\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  2  0  0  0]\n",
      " [ 0  4  0  0  0]\n",
      " [ 0  1  8  1  2]\n",
      " [ 0  0  5  5  4]\n",
      " [ 0  1  2  4 14]]\n",
      "\n",
      "ðŸ” HUBERT FEATURE ANALYSIS:\n",
      "  Original HuBERT feature dimension: 98304\n",
      "  Reduced feature dimension: 187\n",
      "  Dimensionality reduction: 525.7x\n",
      "  Feature composition: 100% HuBERT representations\n",
      "  Multi-layer strategy: 4 layers\n",
      "  Pooling strategies: 4 methods\n",
      "\n",
      "================================================================================\n",
      "ðŸ“ˆ GOOD PERFORMANCE WITH PURE HUBERT!\n",
      "F1 Score: 0.4718\n",
      "Improvement over baseline: +-0.0201 (+-4.1%)\n",
      "ðŸ’¡ Consider adding statistical features for potential improvement\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: EVALUATE ON VALIDATION SET - HUBERT ONLY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[8/9] VALIDATION SET EVALUATION - PURE HUBERT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predict\n",
    "y_val_pred = classifier.predict(X_val_scaled)\n",
    "y_val_pred_proba = classifier.predict_proba(X_val_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
    "f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "precision = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_val, y_val_pred)\n",
    "f1_per_class = f1_score(y_val, y_val_pred, average=None, zero_division=0)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ FINAL RESULTS - PURE HUBERT FEATURES + BALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Feature Strategy: 100% HUBERT REPRESENTATIONS\")\n",
    "print(f\"Statistical Features: {config.USE_STATISTICAL_FEATURES} (DISABLED)\")\n",
    "print(f\"Dataset Strategy: BALANCED\")\n",
    "print(f\"Classifier: SVM (C={config.SVM_CONFIG['C']})\")\n",
    "print(f\"Validation Accuracy:      {accuracy:.4f}\")\n",
    "print(f\"F1 Score (Macro):         {f1_macro:.4f} â­\")\n",
    "print(f\"F1 Score (Weighted):      {f1_weighted:.4f}\")\n",
    "print(f\"Precision (Macro):        {precision:.4f}\")\n",
    "print(f\"Recall (Macro):           {recall:.4f}\")\n",
    "print(f\"Cohen's Kappa:            {kappa:.4f}\")\n",
    "print(f\"CV F1 (Mean Â± Std):       {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nðŸ“Š PER-CLASS PERFORMANCE (PURE HUBERT FEATURES):\")\n",
    "available_classes = sorted(np.unique(y_val))\n",
    "for cls, f1_val in zip(available_classes, [f1_per_class[i] for i in available_classes]):\n",
    "    label = config.CLASS_NAMES[cls]\n",
    "    bar = 'â–ˆ' * int(f1_val * 40)\n",
    "    print(f\"  Class {cls} ({label:25s}): {f1_val:.4f} {bar}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"-\"*80)\n",
    "class_labels = [config.CLASS_NAMES[i] for i in available_classes]\n",
    "print(classification_report(y_val, y_val_pred, target_names=class_labels, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"\\nCONFUSION MATRIX:\")\n",
    "print(cm)\n",
    "\n",
    "# HuBERT feature analysis\n",
    "print(f\"\\nðŸ” HUBERT FEATURE ANALYSIS:\")\n",
    "print(f\"  Original HuBERT feature dimension: {X_train.shape[1]}\")\n",
    "if pca:\n",
    "    print(f\"  Reduced feature dimension: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"  Dimensionality reduction: {X_train.shape[1]/X_train_scaled.shape[1]:.1f}x\")\n",
    "print(f\"  Feature composition: 100% HuBERT representations\")\n",
    "print(f\"  Multi-layer strategy: {len(config.LAYERS_TO_USE)} layers\")\n",
    "print(f\"  Pooling strategies: {len(config.POOLING_STRATEGIES)} methods\")\n",
    "\n",
    "# Achievement status\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "baseline_f1 = 0.4919\n",
    "target_f1 = 0.70\n",
    "improvement = f1_macro - baseline_f1\n",
    "\n",
    "if f1_macro >= target_f1:\n",
    "    print(f\"ðŸŽ‰ðŸŽ‰ðŸŽ‰ TARGET ACHIEVED WITH PURE HUBERT! ðŸŽ‰ðŸŽ‰ðŸŽ‰\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f} >= {target_f1:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "    print(f\"âœ… Pure HuBERT features are sufficient for high performance!\")\n",
    "elif f1_macro >= 0.65:\n",
    "    print(f\"ðŸš€ EXCELLENT PERFORMANCE WITH PURE HUBERT!\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"Gap to target: -{target_f1 - f1_macro:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"ðŸ“ˆ GOOD PERFORMANCE WITH PURE HUBERT!\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "    print(f\"ðŸ’¡ Consider adding statistical features for potential improvement\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a8458c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[9/9] SAVING RESULTS - PURE HUBERT FEATURES\n",
      "================================================================================\n",
      "ðŸ’¾ Saving models...\n",
      "  âœ… Scaler saved\n",
      "  âœ… PCA saved\n",
      "  âœ… Classifier saved\n",
      "  âœ… Results saved: SAND_Project_Data\\hubert_only_balanced\\results.json\n",
      "  âœ… Predictions saved: SAND_Project_Data\\hubert_only_balanced\\validation_predictions.csv\n",
      "\n",
      "âœ… All results saved to: SAND_Project_Data\\hubert_only_balanced\n",
      "\n",
      "ðŸŽ¯ PURE HUBERT STRATEGY SUMMARY:\n",
      "  â€¢ Feature Strategy: 100% HuBERT representations\n",
      "  â€¢ Statistical Features: DISABLED\n",
      "  â€¢ Dataset: BALANCED (SMOTE)\n",
      "  â€¢ Multi-layer extraction: 4 layers\n",
      "  â€¢ Multi-pooling strategies: 4 methods\n",
      "  â€¢ Feature dimension: 98304 â†’ 187\n",
      "  â€¢ Final F1 Score: 0.4718\n",
      "  â€¢ Pure HuBERT performance: ðŸ“ˆ GOOD\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: SAVE ALL MODELS AND RESULTS - HUBERT ONLY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[9/9] SAVING RESULTS - PURE HUBERT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save models\n",
    "print(\"ðŸ’¾ Saving models...\")\n",
    "joblib.dump(scaler, os.path.join(config.OUTPUT_PATH, 'scaler.pkl'))\n",
    "print(f\"  âœ… Scaler saved\")\n",
    "\n",
    "if pca:\n",
    "    joblib.dump(pca, os.path.join(config.OUTPUT_PATH, 'pca.pkl'))\n",
    "    print(f\"  âœ… PCA saved\")\n",
    "\n",
    "joblib.dump(classifier, os.path.join(config.OUTPUT_PATH, 'classifier.pkl'))\n",
    "print(f\"  âœ… Classifier saved\")\n",
    "\n",
    "# Save results dictionary with HuBERT-only focus\n",
    "results = {\n",
    "    'model': 'Pure HuBERT Features + Balanced Dataset',\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'strategy': 'HuBERT Only + Balanced Dataset',\n",
    "    'feature_strategy': 'pure_hubert',\n",
    "    'statistical_features': False,\n",
    "    'classifier_type': 'SVM',\n",
    "    'dataset_strategy': 'balanced',\n",
    "    'metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'kappa': float(kappa),\n",
    "        'cv_mean': float(cv_scores.mean()),\n",
    "        'cv_std': float(cv_scores.std()),\n",
    "    },\n",
    "    'f1_per_class': {int(i): float(f1_per_class[i]) for i in range(len(f1_per_class))},\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'hubert_feature_analysis': {\n",
    "        'original_dimension': int(X_train.shape[1]),\n",
    "        'final_dimension': int(X_train_scaled.shape[1]) if pca else int(X_train.shape[1]),\n",
    "        'reduction_factor': float(X_train.shape[1] / X_train_scaled.shape[1]) if pca else 1.0,\n",
    "        'layers_used': len(config.LAYERS_TO_USE),\n",
    "        'pooling_strategies': len(config.POOLING_STRATEGIES),\n",
    "        'feature_composition': '100%_hubert'\n",
    "    },\n",
    "    'configuration': {\n",
    "        'hubert_model': config.HUBERT_MODEL,\n",
    "        'multi_layer': config.USE_MULTI_LAYER,\n",
    "        'n_layers': len(config.LAYERS_TO_USE),\n",
    "        'layers': config.LAYERS_TO_USE,\n",
    "        'multi_pooling': config.USE_MULTI_POOLING,\n",
    "        'pooling_strategies': config.POOLING_STRATEGIES,\n",
    "        'statistical_features': config.USE_STATISTICAL_FEATURES,\n",
    "        'robust_scaling': config.USE_ROBUST_SCALING,\n",
    "        'pca': config.USE_PCA,\n",
    "        'pca_variance': config.PCA_VARIANCE if config.USE_PCA else None,\n",
    "        'balancing_strategy': 'smote' if config.USE_SMOTE else 'none',\n",
    "        'svm_C': config.SVM_CONFIG['C'],\n",
    "        'svm_kernel': config.SVM_CONFIG['kernel'],\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = os.path.join(config.OUTPUT_PATH, 'results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "print(f\"  âœ… Results saved: {results_path}\")\n",
    "\n",
    "# Save predictions\n",
    "val_predictions = pd.DataFrame({\n",
    "    'ID': ids_val,\n",
    "    'True_Class': y_val,\n",
    "    'Predicted_Class': y_val_pred,\n",
    "    'True_Label': [config.CLASS_NAMES[int(c)] for c in y_val],\n",
    "    'Predicted_Label': [config.CLASS_NAMES[int(c)] for c in y_val_pred],\n",
    "    'Correct': (y_val == y_val_pred).astype(int),\n",
    "    'Confidence': y_val_pred_proba.max(axis=1)\n",
    "})\n",
    "\n",
    "# Add probability columns\n",
    "for cls in range(len(config.CLASS_NAMES)):\n",
    "    val_predictions[f'Prob_Class_{cls}'] = y_val_pred_proba[:, cls]\n",
    "    val_predictions[f'Prob_{config.CLASS_NAMES[cls]}'] = y_val_pred_proba[:, cls]\n",
    "\n",
    "val_predictions = val_predictions.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "pred_path = os.path.join(config.OUTPUT_PATH, 'validation_predictions.csv')\n",
    "val_predictions.to_csv(pred_path, index=False)\n",
    "print(f\"  âœ… Predictions saved: {pred_path}\")\n",
    "\n",
    "print(f\"\\nâœ… All results saved to: {config.OUTPUT_PATH}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nðŸŽ¯ PURE HUBERT STRATEGY SUMMARY:\")\n",
    "print(f\"  â€¢ Feature Strategy: 100% HuBERT representations\")\n",
    "print(f\"  â€¢ Statistical Features: DISABLED\")\n",
    "print(f\"  â€¢ Dataset: BALANCED (SMOTE)\")\n",
    "print(f\"  â€¢ Multi-layer extraction: {len(config.LAYERS_TO_USE)} layers\")\n",
    "print(f\"  â€¢ Multi-pooling strategies: {len(config.POOLING_STRATEGIES)} methods\")\n",
    "print(f\"  â€¢ Feature dimension: {X_train.shape[1]} â†’ {X_train_scaled.shape[1] if pca else X_train.shape[1]}\")\n",
    "print(f\"  â€¢ Final F1 Score: {f1_macro:.4f}\")\n",
    "print(f\"  â€¢ Pure HuBERT performance: {'âœ… SUFFICIENT' if f1_macro >= 0.65 else 'ðŸ“ˆ GOOD'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
