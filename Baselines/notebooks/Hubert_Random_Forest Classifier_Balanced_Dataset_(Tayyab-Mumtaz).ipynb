{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ea4711",
   "metadata": {},
   "source": [
    "# Hubert Random Forest Classifier Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e068af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ OPTIMIZED HUBERT + RANDOM FOREST FOR SAND CHALLENGE\n",
      "Strategy: HuBERT + Random Forest + Balanced Dataset\n",
      "Target: F1 Score >= 0.70 | High Accuracy\n",
      "================================================================================\n",
      "âœ… Using device: cpu\n",
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ OPTIMIZED HUBERT + RANDOM FOREST FOR SAND CHALLENGE\")\n",
    "print(\"Strategy: HuBERT + Random Forest + Balanced Dataset\")\n",
    "print(\"Target: F1 Score >= 0.70 | High Accuracy\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, cohen_kappa_score\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a70c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“‹ OPTIMIZED CONFIGURATION - RANDOM FOREST\n",
      "================================================================================\n",
      "HuBERT Model: facebook/hubert-base-ls960\n",
      "Classifier: Random Forest (n_estimators=500)\n",
      "Multi-Layer Extraction: True (4 layers)\n",
      "Multi-Pooling: True (4 strategies)\n",
      "Statistical Features: True\n",
      "PCA: True (variance=0.97)\n",
      "Balancing Strategy: balanced_rf\n",
      "Class Weight: balanced_subsample\n",
      "Device: cpu\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Verifying paths...\n",
      "âœ… All paths verified!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: OPTIMIZED CONFIGURATION FOR RANDOM FOREST\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    \"\"\"Optimized Configuration for Random Forest with Balanced Dataset\"\"\"\n",
    "\n",
    "    # Paths - UPDATE THESE PATHS FOR YOUR LOCAL SYSTEM\n",
    "    BASE_PATH = 'SAND_Project_Data'  # Update this path\n",
    "    TRAINING_PATH = os.path.join(BASE_PATH, 'training')\n",
    "    EXCEL_PATH = os.path.join(BASE_PATH, 'sand_task_1.xlsx')\n",
    "    OUTPUT_PATH = os.path.join(BASE_PATH, 'optimized_hubert_rf_balanced')\n",
    "\n",
    "    # Sheet names\n",
    "    SHEET_TRAINING = 'Training Baseline - Task 1'\n",
    "    SHEET_VALIDATION = 'Validation Baseline - Task 1'\n",
    "\n",
    "    # Audio types\n",
    "    AUDIO_TYPES = ['phonationA', 'phonationE', 'phonationI', 'phonationO', 'phonationU',\n",
    "                   'rhythmKA', 'rhythmPA', 'rhythmTA']\n",
    "\n",
    "    # ========================================================================\n",
    "    # OPTIMIZED HUBERT CONFIGURATION\n",
    "    # ========================================================================\n",
    "    HUBERT_MODEL = 'facebook/hubert-base-ls960'\n",
    "    SAMPLING_RATE = 16000\n",
    "    MAX_AUDIO_LENGTH = 16000 * 15  # 15 seconds\n",
    "\n",
    "    # Multi-layer feature extraction (CRITICAL FOR PERFORMANCE)\n",
    "    USE_MULTI_LAYER = True\n",
    "    LAYERS_TO_USE = [-4, -3, -2, -1]  # Last 4 layers\n",
    "\n",
    "    # Multi-pooling strategy (CRITICAL FOR PERFORMANCE)\n",
    "    USE_MULTI_POOLING = True\n",
    "    POOLING_STRATEGIES = ['mean', 'std', 'max', 'min']  # 4 pooling methods\n",
    "\n",
    "    # Statistical features (HELPS WITH DYSARTHRIA PATTERNS)\n",
    "    USE_STATISTICAL_FEATURES = True\n",
    "\n",
    "    # Audio augmentation during loading\n",
    "    AUDIO_TRIM_DB = 15  # Lower threshold for dysarthric speech\n",
    "    AUDIO_NORMALIZE = True\n",
    "    MIN_AUDIO_LENGTH_SEC = 2\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREPROCESSING OPTIMIZATIONS\n",
    "    # ========================================================================\n",
    "    USE_ROBUST_SCALING = True  # Better for outliers\n",
    "\n",
    "    # PCA Configuration\n",
    "    USE_PCA = True\n",
    "    PCA_VARIANCE = 0.97  # Retain 97% variance\n",
    "\n",
    "    # ========================================================================\n",
    "    # DATA BALANCING STRATEGIES (CRITICAL FOR RANDOM FOREST)\n",
    "    # ========================================================================\n",
    "    BALANCING_STRATEGY = 'balanced_rf'  # Options: 'smote', 'balanced_rf', 'hybrid'\n",
    "    USE_SMOTE = True\n",
    "    SMOTE_STRATEGY = 'auto'  # Automatically balance all classes\n",
    "    SMOTE_K_NEIGHBORS = 3\n",
    "\n",
    "    # ========================================================================\n",
    "    # OPTIMIZED RANDOM FOREST HYPERPARAMETERS\n",
    "    # ========================================================================\n",
    "    RF_CONFIG = {\n",
    "        'n_estimators': 500,           # More trees for better performance\n",
    "        'criterion': 'gini',           # Split quality measure\n",
    "        'max_depth': 25,               # Limit tree depth to prevent overfitting\n",
    "        'min_samples_split': 5,        # Minimum samples to split a node\n",
    "        'min_samples_leaf': 2,         # Minimum samples at a leaf node\n",
    "        'max_features': 'sqrt',        # Features to consider for best split\n",
    "        'bootstrap': True,             # Use bootstrap sampling\n",
    "        'class_weight': 'balanced_subsample',  # Balance classes in each tree\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': -1,                  # Use all available cores\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Alternative: Balanced Random Forest (if using imbalanced-learn)\n",
    "    BALANCED_RF_CONFIG = {\n",
    "        'n_estimators': 400,\n",
    "        'criterion': 'gini',\n",
    "        'max_depth': 20,\n",
    "        'min_samples_split': 4,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'log2',\n",
    "        'sampling_strategy': 'auto',  # Automatically balance classes\n",
    "        'replacement': True,          # Sample with replacement\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    N_FOLDS = 5\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # Class names\n",
    "    CLASS_NAMES = {\n",
    "        0: 'Severe Dysarthria',\n",
    "        1: 'Moderate Dysarthria',\n",
    "        2: 'Mild Dysarthria',\n",
    "        3: 'No Dysarthria (ALS)',\n",
    "        4: 'Healthy'\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ OPTIMIZED CONFIGURATION - RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"HuBERT Model: {config.HUBERT_MODEL}\")\n",
    "print(f\"Classifier: Random Forest (n_estimators={config.RF_CONFIG['n_estimators']})\")\n",
    "print(f\"Multi-Layer Extraction: {config.USE_MULTI_LAYER} ({len(config.LAYERS_TO_USE)} layers)\")\n",
    "print(f\"Multi-Pooling: {config.USE_MULTI_POOLING} ({len(config.POOLING_STRATEGIES)} strategies)\")\n",
    "print(f\"Statistical Features: {config.USE_STATISTICAL_FEATURES}\")\n",
    "print(f\"PCA: {config.USE_PCA} (variance={config.PCA_VARIANCE})\")\n",
    "print(f\"Balancing Strategy: {config.BALANCING_STRATEGY}\")\n",
    "print(f\"Class Weight: {config.RF_CONFIG['class_weight']}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"\\nðŸ” Verifying paths...\")\n",
    "assert os.path.exists(config.BASE_PATH), f\"Base path not found: {config.BASE_PATH}\"\n",
    "assert os.path.exists(config.TRAINING_PATH), f\"Training path not found: {config.TRAINING_PATH}\"\n",
    "assert os.path.exists(config.EXCEL_PATH), f\"Excel file not found: {config.EXCEL_PATH}\"\n",
    "print(\"âœ… All paths verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cc5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1/9] LOADING DATASETS\n",
      "================================================================================\n",
      "\n",
      "âœ… Loaded 219 samples from: 'Training Baseline - Task 1'\n",
      "ðŸ“Š Class distribution:\n",
      "  Class 0 (Severe Dysarthria        ):   4 ( 1.83%)\n",
      "  Class 1 (Moderate Dysarthria      ):  22 (10.05%)\n",
      "  Class 2 (Mild Dysarthria          ):  45 (20.55%)\n",
      "  Class 3 (No Dysarthria (ALS)      ):  62 (28.31%)\n",
      "  Class 4 (Healthy                  ):  86 (39.27%)\n",
      "\n",
      "âœ… Loaded 53 samples from: 'Validation Baseline - Task 1'\n",
      "ðŸ“Š Class distribution:\n",
      "  Class 0 (Severe Dysarthria        ):   2 ( 3.77%)\n",
      "  Class 1 (Moderate Dysarthria      ):   4 ( 7.55%)\n",
      "  Class 2 (Mild Dysarthria          ):  12 (22.64%)\n",
      "  Class 3 (No Dysarthria (ALS)      ):  14 (26.42%)\n",
      "  Class 4 (Healthy                  ):  21 (39.62%)\n",
      "\n",
      "ðŸ“ˆ Summary:\n",
      "  Training samples: 219\n",
      "  Validation samples: 53\n",
      "  Total audio files: 2176\n",
      "  Overlap check: âœ… No overlap\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: LOAD DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[1/9] LOADING DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_data(excel_path, sheet_name):\n",
    "    \"\"\"Load dataset from Excel\"\"\"\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    print(f\"\\nâœ… Loaded {len(df)} samples from: '{sheet_name}'\")\n",
    "\n",
    "    # Convert to 0-indexed classes\n",
    "    df['Class'] = df['Class'] - 1\n",
    "\n",
    "    # Display distribution\n",
    "    print(f\"ðŸ“Š Class distribution:\")\n",
    "    class_dist = df['Class'].value_counts().sort_index()\n",
    "    for cls, count in class_dist.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  Class {cls} ({config.CLASS_NAMES[cls]:25s}): {count:3d} ({pct:5.2f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load training and validation sets\n",
    "df_train = load_data(config.EXCEL_PATH, config.SHEET_TRAINING)\n",
    "df_val = load_data(config.EXCEL_PATH, config.SHEET_VALIDATION)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Summary:\")\n",
    "print(f\"  Training samples: {len(df_train)}\")\n",
    "print(f\"  Validation samples: {len(df_val)}\")\n",
    "print(f\"  Total audio files: {(len(df_train) + len(df_val)) * len(config.AUDIO_TYPES)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_ids = set(df_train['ID'].values)\n",
    "val_ids = set(df_val['ID'].values)\n",
    "overlap = train_ids.intersection(val_ids)\n",
    "print(f\"  Overlap check: {'âœ… No overlap' if len(overlap) == 0 else f'âš  {len(overlap)} overlapping IDs'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ec0f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2/9] LOADING HUBERT MODEL\n",
      "================================================================================\n",
      "Model: facebook/hubert-base-ls960\n",
      "â± This may take 1-2 minutes for first-time download...\n",
      "\n",
      "âœ… HuBERT loaded successfully!\n",
      "  Model size: 94.4M parameters\n",
      "  Hidden size: 768\n",
      "  Number of layers: 12\n",
      "  Sampling rate: 16000 Hz\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: LOAD HUBERT MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2/9] LOADING HUBERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {config.HUBERT_MODEL}\")\n",
    "print(\"â± This may take 1-2 minutes for first-time download...\")\n",
    "\n",
    "# Load feature extractor and model\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.HUBERT_MODEL)\n",
    "hubert_model = HubertModel.from_pretrained(config.HUBERT_MODEL)\n",
    "hubert_model = hubert_model.to(device)\n",
    "hubert_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"\\nâœ… HuBERT loaded successfully!\")\n",
    "print(f\"  Model size: {sum(p.numel() for p in hubert_model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"  Hidden size: {hubert_model.config.hidden_size}\")\n",
    "print(f\"  Number of layers: {hubert_model.config.num_hidden_layers}\")\n",
    "print(f\"  Sampling rate: {feature_extractor.sampling_rate} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38bdb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[3/9] DEFINING ENHANCED AUDIO PROCESSING\n",
      "================================================================================\n",
      "âœ… Audio processing functions defined!\n",
      "  Enhanced audio loading: âœ“\n",
      "  Statistical features (20 features): âœ“\n",
      "  Multi-layer HuBERT extraction: âœ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: ENHANCED AUDIO PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3/9] DEFINING ENHANCED AUDIO PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_audio_enhanced(audio_path, target_sr=16000, max_length=None):\n",
    "    \"\"\"\n",
    "    Enhanced audio loading with advanced preprocessing\n",
    "\n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        target_sr: Target sampling rate\n",
    "        max_length: Maximum audio length in samples\n",
    "\n",
    "    Returns:\n",
    "        audio: Preprocessed audio waveform\n",
    "        sr: Sampling rate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path, sr=target_sr, mono=True)\n",
    "\n",
    "        # Advanced trimming with lower threshold for dysarthric speech\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=config.AUDIO_TRIM_DB)\n",
    "\n",
    "        # Normalize amplitude\n",
    "        if config.AUDIO_NORMALIZE:\n",
    "            audio = librosa.util.normalize(audio)\n",
    "\n",
    "        # Handle length constraints\n",
    "        if max_length and len(audio) > max_length:\n",
    "            audio = audio[:max_length]\n",
    "\n",
    "        # Ensure minimum length\n",
    "        min_length = target_sr * config.MIN_AUDIO_LENGTH_SEC\n",
    "        if len(audio) < min_length:\n",
    "            audio = np.pad(audio, (0, min_length - len(audio)), mode='constant')\n",
    "\n",
    "        return audio, sr\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error loading {audio_path}: {str(e)}\")\n",
    "        # Return silence as fallback\n",
    "        return np.zeros(target_sr * config.MIN_AUDIO_LENGTH_SEC), target_sr\n",
    "\n",
    "def extract_statistical_features(audio, sr=16000):\n",
    "    \"\"\"\n",
    "    Extract statistical and acoustic features from audio\n",
    "    These help capture dysarthria-specific patterns\n",
    "\n",
    "    Args:\n",
    "        audio: Audio waveform\n",
    "        sr: Sampling rate\n",
    "\n",
    "    Returns:\n",
    "        features: Array of statistical features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Time-domain statistics\n",
    "    features.extend([\n",
    "        np.mean(audio),           # Mean amplitude\n",
    "        np.std(audio),            # Standard deviation\n",
    "        skew(audio),              # Skewness\n",
    "        kurtosis(audio),          # Kurtosis\n",
    "        np.max(np.abs(audio)),    # Peak amplitude\n",
    "        np.median(audio),         # Median\n",
    "        np.percentile(audio, 25), # 25th percentile\n",
    "        np.percentile(audio, 75)  # 75th percentile\n",
    "    ])\n",
    "\n",
    "    # Zero crossing rate (voice quality indicator)\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio)[0]\n",
    "    features.extend([np.mean(zcr), np.std(zcr), np.max(zcr)])\n",
    "\n",
    "    # Energy features\n",
    "    energy = np.sum(audio**2) / len(audio)\n",
    "    features.append(energy)\n",
    "\n",
    "    # RMS energy\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "    # Spectral features\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
    "    features.extend([np.mean(spectral_centroids), np.std(spectral_centroids)])\n",
    "\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
    "    features.extend([np.mean(spectral_rolloff), np.std(spectral_rolloff)])\n",
    "\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]\n",
    "    features.extend([np.mean(spectral_bandwidth), np.std(spectral_bandwidth)])\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_hubert_features_multilayer(audio, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    Extract HuBERT features with multi-layer and multi-pooling strategy\n",
    "    This is the KEY to high performance\n",
    "\n",
    "    Args:\n",
    "        audio: Audio waveform\n",
    "        sampling_rate: Sampling rate\n",
    "\n",
    "    Returns:\n",
    "        features: Concatenated feature vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess audio for HuBERT\n",
    "        inputs = feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Extract features with all hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = hubert_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        all_features = []\n",
    "\n",
    "        # Extract from multiple layers\n",
    "        if config.USE_MULTI_LAYER:\n",
    "            layers_to_process = config.LAYERS_TO_USE\n",
    "        else:\n",
    "            layers_to_process = [-1]  # Only last layer\n",
    "\n",
    "        for layer_idx in layers_to_process:\n",
    "            hidden_state = outputs.hidden_states[layer_idx]  # Shape: [batch, time, hidden]\n",
    "\n",
    "            # Apply multiple pooling strategies\n",
    "            if config.USE_MULTI_POOLING:\n",
    "                pooling_methods = config.POOLING_STRATEGIES\n",
    "            else:\n",
    "                pooling_methods = ['mean']  # Only mean pooling\n",
    "\n",
    "            for pool_strategy in pooling_methods:\n",
    "                if pool_strategy == 'mean':\n",
    "                    pooled = hidden_state.mean(dim=1)\n",
    "                elif pool_strategy == 'max':\n",
    "                    pooled = hidden_state.max(dim=1)[0]\n",
    "                elif pool_strategy == 'std':\n",
    "                    pooled = hidden_state.std(dim=1)\n",
    "                elif pool_strategy == 'min':\n",
    "                    pooled = hidden_state.min(dim=1)[0]\n",
    "                else:\n",
    "                    pooled = hidden_state.mean(dim=1)\n",
    "\n",
    "                all_features.append(pooled.cpu().numpy().squeeze())\n",
    "\n",
    "        # Concatenate all features\n",
    "        combined_features = np.concatenate(all_features)\n",
    "\n",
    "        return combined_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Feature extraction error: {str(e)}\")\n",
    "        # Return zero features as fallback\n",
    "        n_layers = len(config.LAYERS_TO_USE) if config.USE_MULTI_LAYER else 1\n",
    "        n_pooling = len(config.POOLING_STRATEGIES) if config.USE_MULTI_POOLING else 1\n",
    "        feature_dim = hubert_model.config.hidden_size * n_layers * n_pooling\n",
    "        return np.zeros(feature_dim)\n",
    "\n",
    "print(\"âœ… Audio processing functions defined!\")\n",
    "print(f\"  Enhanced audio loading: âœ“\")\n",
    "print(f\"  Statistical features ({20} features): âœ“\")\n",
    "print(f\"  Multi-layer HuBERT extraction: âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe2d752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[4/9] EXTRACTING FEATURES\n",
      "================================================================================\n",
      "Configuration:\n",
      "  Layers: 4\n",
      "  Pooling strategies: 4\n",
      "  Statistical features: True\n",
      "  Feature multiplication factor: 16x\n",
      "\n",
      "ðŸŽµ Training 219 patients...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a4e33f56af4ec59c30e90b721088bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽµ Validation 53 patients...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4a9249642542bdb861040f3214f671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Feature extraction complete!\n",
      "  Training shape: (219, 98464)\n",
      "  Validation shape: (53, 98464)\n",
      "  Feature dimension per patient: 98464\n",
      "  Data cleaned: âœ“\n",
      "  ðŸ’¾ Raw features saved: SAND_Project_Data\\optimized_hubert_rf_balanced\\features_raw.npz\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: EXTRACT FEATURES FROM ALL AUDIO FILES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4/9] EXTRACTING FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Layers: {len(config.LAYERS_TO_USE)}\")\n",
    "print(f\"  Pooling strategies: {len(config.POOLING_STRATEGIES)}\")\n",
    "print(f\"  Statistical features: {config.USE_STATISTICAL_FEATURES}\")\n",
    "print(f\"  Feature multiplication factor: {len(config.LAYERS_TO_USE) * len(config.POOLING_STRATEGIES)}x\")\n",
    "\n",
    "def extract_features_for_dataset(df, dataset_path, audio_types, desc=\"Processing\"):\n",
    "    \"\"\"\n",
    "    Extract enhanced features for entire dataset\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with patient IDs and labels\n",
    "        dataset_path: Path to audio files\n",
    "        audio_types: List of audio types\n",
    "        desc: Progress bar description\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        ids: Patient IDs\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    ids_list = []\n",
    "\n",
    "    print(f\"\\nðŸŽµ {desc} {len(df)} patients...\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        patient_id = row['ID']\n",
    "        patient_class = row['Class']\n",
    "\n",
    "        patient_features = []\n",
    "\n",
    "        # Process each audio type\n",
    "        for audio_type in audio_types:\n",
    "            audio_file = f\"{patient_id}_{audio_type}.wav\"\n",
    "            audio_path = os.path.join(dataset_path, audio_type, audio_file)\n",
    "\n",
    "            if os.path.exists(audio_path):\n",
    "                # Load audio with enhancements\n",
    "                audio, sr = load_audio_enhanced(\n",
    "                    audio_path,\n",
    "                    target_sr=config.SAMPLING_RATE,\n",
    "                    max_length=config.MAX_AUDIO_LENGTH\n",
    "                )\n",
    "\n",
    "                # Extract HuBERT features\n",
    "                hubert_features = extract_hubert_features_multilayer(audio, sr)\n",
    "\n",
    "                # Extract statistical features if enabled\n",
    "                if config.USE_STATISTICAL_FEATURES:\n",
    "                    stat_features = extract_statistical_features(audio, sr)\n",
    "                    combined_features = np.concatenate([hubert_features, stat_features])\n",
    "                else:\n",
    "                    combined_features = hubert_features\n",
    "\n",
    "                patient_features.append(combined_features)\n",
    "            else:\n",
    "                # Zero features for missing files\n",
    "                n_layers = len(config.LAYERS_TO_USE) if config.USE_MULTI_LAYER else 1\n",
    "                n_pooling = len(config.POOLING_STRATEGIES) if config.USE_MULTI_POOLING else 1\n",
    "                feature_dim = hubert_model.config.hidden_size * n_layers * n_pooling\n",
    "\n",
    "                if config.USE_STATISTICAL_FEATURES:\n",
    "                    feature_dim += 20  # Statistical features count\n",
    "\n",
    "                patient_features.append(np.zeros(feature_dim))\n",
    "\n",
    "        # Concatenate features from all audio types\n",
    "        combined = np.concatenate(patient_features)\n",
    "        features_list.append(combined)\n",
    "        labels_list.append(patient_class)\n",
    "        ids_list.append(patient_id)\n",
    "\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(labels_list)\n",
    "    ids = np.array(ids_list)\n",
    "\n",
    "    return X, y, ids\n",
    "\n",
    "# Extract training features\n",
    "X_train, y_train, ids_train = extract_features_for_dataset(\n",
    "    df_train,\n",
    "    config.TRAINING_PATH,\n",
    "    config.AUDIO_TYPES,\n",
    "    desc=\"Training\"\n",
    ")\n",
    "\n",
    "# Extract validation features\n",
    "X_val, y_val, ids_val = extract_features_for_dataset(\n",
    "    df_val,\n",
    "    config.TRAINING_PATH,\n",
    "    config.AUDIO_TYPES,\n",
    "    desc=\"Validation\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Feature extraction complete!\")\n",
    "print(f\"  Training shape: {X_train.shape}\")\n",
    "print(f\"  Validation shape: {X_val.shape}\")\n",
    "print(f\"  Feature dimension per patient: {X_train.shape[1]}\")\n",
    "\n",
    "# Clean data (handle any NaN/Inf)\n",
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"  Data cleaned: âœ“\")\n",
    "\n",
    "# Save raw features\n",
    "features_path = os.path.join(config.OUTPUT_PATH, 'features_raw.npz')\n",
    "np.savez(\n",
    "    features_path,\n",
    "    X_train=X_train, y_train=y_train, ids_train=ids_train,\n",
    "    X_val=X_val, y_val=y_val, ids_val=ids_val\n",
    ")\n",
    "print(f\"  ðŸ’¾ Raw features saved: {features_path}\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "729f7868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[5/9] ADVANCED PREPROCESSING FOR RANDOM FOREST\n",
      "================================================================================\n",
      "ðŸ“Š Applying RobustScaler (better for outliers)...\n",
      "  âœ… Scaling complete\n",
      "     Train: mean=0.022045, std=0.781847\n",
      "     Val:   mean=0.033796, std=0.821249\n",
      "\n",
      "ðŸ“‰ Applying PCA (variance threshold=0.97)...\n",
      "  âœ… PCA complete\n",
      "     Original dimensions: 98464\n",
      "     Reduced dimensions: 199\n",
      "     Explained variance: 97.14%\n",
      "     Dimensionality reduction: 494.8x\n",
      "\n",
      "âš–ï¸ Applying Data Balancing Strategy: balanced_rf...\n",
      "  Original class distribution:\n",
      "    Class 0 (Severe Dysarthria        ):   4\n",
      "    Class 1 (Moderate Dysarthria      ):  22\n",
      "    Class 2 (Mild Dysarthria          ):  45\n",
      "    Class 3 (No Dysarthria (ALS)      ):  62\n",
      "    Class 4 (Healthy                  ):  86\n",
      "  Using Random Forest with balanced class weights...\n",
      "\n",
      "  âœ… Balancing complete: Balanced RF\n",
      "     Samples: 219 â†’ 219\n",
      "  Final training distribution:\n",
      "    Class 0:   4\n",
      "    Class 1:  22\n",
      "    Class 2:  45\n",
      "    Class 3:  62\n",
      "    Class 4:  86\n",
      "\n",
      "âœ… Preprocessing pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: ADVANCED PREPROCESSING PIPELINE FOR RANDOM FOREST\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5/9] ADVANCED PREPROCESSING FOR RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Scaling\n",
    "if config.USE_ROBUST_SCALING:\n",
    "    print(\"ðŸ“Š Applying RobustScaler (better for outliers)...\")\n",
    "    scaler = RobustScaler()\n",
    "else:\n",
    "    print(\"ðŸ“Š Applying StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"  âœ… Scaling complete\")\n",
    "print(f\"     Train: mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")\n",
    "print(f\"     Val:   mean={X_val_scaled.mean():.6f}, std={X_val_scaled.std():.6f}\")\n",
    "\n",
    "# Step 2: PCA (Dimensionality Reduction)\n",
    "if config.USE_PCA:\n",
    "    print(f\"\\nðŸ“‰ Applying PCA (variance threshold={config.PCA_VARIANCE})...\")\n",
    "    pca = PCA(n_components=config.PCA_VARIANCE, random_state=config.RANDOM_SEED)\n",
    "    X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "    X_val_scaled = pca.transform(X_val_scaled)\n",
    "\n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"  âœ… PCA complete\")\n",
    "    print(f\"     Original dimensions: {X_train.shape[1]}\")\n",
    "    print(f\"     Reduced dimensions: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"     Explained variance: {explained_var*100:.2f}%\")\n",
    "    print(f\"     Dimensionality reduction: {X_train.shape[1]/X_train_scaled.shape[1]:.1f}x\")\n",
    "else:\n",
    "    pca = None\n",
    "    print(\"  â„¹ PCA disabled\")\n",
    "\n",
    "# Step 3: Data Balancing Strategy\n",
    "print(f\"\\nâš–ï¸ Applying Data Balancing Strategy: {config.BALANCING_STRATEGY}...\")\n",
    "print(f\"  Original class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"    Class {cls} ({config.CLASS_NAMES[cls]:25s}): {count:3d}\")\n",
    "\n",
    "if config.BALANCING_STRATEGY == 'smote':\n",
    "    # Use SMOTE for balancing\n",
    "    print(f\"  Using SMOTE (strategy='{config.SMOTE_STRATEGY}')...\")\n",
    "    try:\n",
    "        smote = SMOTE(\n",
    "            sampling_strategy=config.SMOTE_STRATEGY,\n",
    "            k_neighbors=config.SMOTE_K_NEIGHBORS,\n",
    "            random_state=config.RANDOM_SEED\n",
    "        )\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "        balancing_method = \"SMOTE\"\n",
    "    except Exception as e:\n",
    "        print(f\"  âš  SMOTE failed: {e}\")\n",
    "        print(f\"  Continuing without balancing...\")\n",
    "        X_train_resampled = X_train_scaled\n",
    "        y_train_resampled = y_train\n",
    "        balancing_method = \"None\"\n",
    "\n",
    "elif config.BALANCING_STRATEGY == 'balanced_rf':\n",
    "    # Use Random Forest with built-in class balancing\n",
    "    print(\"  Using Random Forest with balanced class weights...\")\n",
    "    X_train_resampled = X_train_scaled\n",
    "    y_train_resampled = y_train\n",
    "    balancing_method = \"Balanced RF\"\n",
    "\n",
    "elif config.BALANCING_STRATEGY == 'hybrid':\n",
    "    # Combine SMOTE with balanced RF\n",
    "    print(\"  Using Hybrid approach (SMOTE + Balanced RF)...\")\n",
    "    try:\n",
    "        smote = SMOTE(\n",
    "            sampling_strategy=config.SMOTE_STRATEGY,\n",
    "            k_neighbors=config.SMOTE_K_NEIGHBORS,\n",
    "            random_state=config.RANDOM_SEED\n",
    "        )\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "        balancing_method = \"Hybrid (SMOTE + Balanced RF)\"\n",
    "    except Exception as e:\n",
    "        print(f\"  âš  SMOTE failed: {e}\")\n",
    "        print(f\"  Continuing with balanced RF only...\")\n",
    "        X_train_resampled = X_train_scaled\n",
    "        y_train_resampled = y_train\n",
    "        balancing_method = \"Balanced RF\"\n",
    "\n",
    "else:\n",
    "    # No balancing\n",
    "    X_train_resampled = X_train_scaled\n",
    "    y_train_resampled = y_train\n",
    "    balancing_method = \"None\"\n",
    "\n",
    "print(f\"\\n  âœ… Balancing complete: {balancing_method}\")\n",
    "print(f\"     Samples: {len(y_train)} â†’ {len(y_train_resampled)}\")\n",
    "print(f\"  Final training distribution:\")\n",
    "unique, counts = np.unique(y_train_resampled, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"    Class {cls}: {count:3d}\")\n",
    "\n",
    "print(\"\\nâœ… Preprocessing pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93e303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[6/9] TRAINING OPTIMIZED RANDOM FOREST\n",
      "================================================================================\n",
      "Configuration:\n",
      "  n_estimators: 500\n",
      "  criterion: gini\n",
      "  max_depth: 25\n",
      "  min_samples_split: 5\n",
      "  min_samples_leaf: 2\n",
      "  max_features: sqrt\n",
      "  bootstrap: True\n",
      "  class_weight: balanced_subsample\n",
      "  random_state: 42\n",
      "  verbose: 0\n",
      "\n",
      "ðŸŒ² Training Random Forest on 219 samples...\n",
      "   Balancing strategy: Balanced RF\n",
      "  Using BalancedRandomForestClassifier from imbalanced-learn...\n",
      "âœ… Random Forest training complete!\n",
      "  Number of trees: 400\n",
      "  Number of features: 199\n",
      "  Feature importance range: 0.001260 - 0.025530\n",
      "  Top 10 feature importances: [0.02553038 0.02483392 0.0185243  0.01728794 0.01395972 0.01269205\n",
      " 0.01182052 0.01079879 0.01032504 0.01001125]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: TRAIN OPTIMIZED RANDOM FOREST CLASSIFIER\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6/9] TRAINING OPTIMIZED RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration:\")\n",
    "for key, value in config.RF_CONFIG.items():\n",
    "    if key != 'n_jobs':  # Skip n_jobs for cleaner output\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸŒ² Training Random Forest on {len(y_train_resampled)} samples...\")\n",
    "print(f\"   Balancing strategy: {balancing_method}\")\n",
    "\n",
    "# Choose the appropriate Random Forest implementation\n",
    "if config.BALANCING_STRATEGY == 'balanced_rf' and 'BalancedRandomForestClassifier' in globals():\n",
    "    print(\"  Using BalancedRandomForestClassifier from imbalanced-learn...\")\n",
    "    classifier = BalancedRandomForestClassifier(**config.BALANCED_RF_CONFIG)\n",
    "else:\n",
    "    print(\"  Using standard RandomForestClassifier with class weighting...\")\n",
    "    classifier = RandomForestClassifier(**config.RF_CONFIG)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(f\"âœ… Random Forest training complete!\")\n",
    "print(f\"  Number of trees: {classifier.n_estimators}\")\n",
    "print(f\"  Number of features: {classifier.n_features_in_}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if hasattr(classifier, 'feature_importances_'):\n",
    "    importances = classifier.feature_importances_\n",
    "    print(f\"  Feature importance range: {importances.min():.6f} - {importances.max():.6f}\")\n",
    "    print(f\"  Top 10 feature importances: {np.sort(importances)[-10:][::-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4483a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[7/9] CROSS-VALIDATION\n",
      "================================================================================\n",
      "Performing 5-fold stratified cross-validation...\n",
      "â± This may take a few minutes for Random Forest...\n",
      "\n",
      "âœ… Cross-validation complete!\n",
      "  Fold scores: ['0.1172', '0.1933', '0.1071', '0.1770', '0.1823']\n",
      "  Mean CV F1: 0.1554\n",
      "  Std CV F1:  0.0358\n",
      "  Min CV F1:  0.1071\n",
      "  Max CV F1:  0.1933\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: CROSS-VALIDATION ON TRAINING SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7/9] CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Performing {config.N_FOLDS}-fold stratified cross-validation...\")\n",
    "print(\"â± This may take a few minutes for Random Forest...\")\n",
    "\n",
    "# Cross-validation on ORIGINAL training set\n",
    "cv_classifier = RandomForestClassifier(**config.RF_CONFIG)\n",
    "cv_scores = cross_val_score(\n",
    "    cv_classifier,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=config.N_FOLDS,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Cross-validation complete!\")\n",
    "print(f\"  Fold scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "print(f\"  Mean CV F1: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std CV F1:  {cv_scores.std():.4f}\")\n",
    "print(f\"  Min CV F1:  {cv_scores.min():.4f}\")\n",
    "print(f\"  Max CV F1:  {cv_scores.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "925d6cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[8/9] VALIDATION SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ FINAL RESULTS - HUBERT + RANDOM FOREST + BALANCED DATASET\n",
      "================================================================================\n",
      "Classifier: Random Forest (n_estimators=400)\n",
      "Balancing Strategy: Balanced RF\n",
      "Validation Accuracy:      0.1698\n",
      "F1 Score (Macro):         0.1645 â­\n",
      "F1 Score (Weighted):      0.1647\n",
      "Precision (Macro):        0.2248\n",
      "Recall (Macro):           0.3595\n",
      "Cohen's Kappa:            0.0478\n",
      "CV F1 (Mean Â± Std):       0.1554 Â± 0.0358\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PER-CLASS PERFORMANCE:\n",
      "  Class 0 (Severe Dysarthria        ): 0.1667 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Class 1 (Moderate Dysarthria      ): 0.2105 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Class 2 (Mild Dysarthria          ): 0.1176 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Class 3 (No Dysarthria (ALS)      ): 0.1053 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Class 4 (Healthy                  ): 0.2222 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CLASSIFICATION REPORT:\n",
      "--------------------------------------------------------------------------------\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  Severe Dysarthria       0.09      1.00      0.17         2\n",
      "Moderate Dysarthria       0.13      0.50      0.21         4\n",
      "    Mild Dysarthria       0.20      0.08      0.12        12\n",
      "No Dysarthria (ALS)       0.20      0.07      0.11        14\n",
      "            Healthy       0.50      0.14      0.22        21\n",
      "\n",
      "           accuracy                           0.17        53\n",
      "          macro avg       0.22      0.36      0.16        53\n",
      "       weighted avg       0.31      0.17      0.16        53\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[2 0 0 0 0]\n",
      " [2 2 0 0 0]\n",
      " [4 6 1 0 1]\n",
      " [6 4 1 1 2]\n",
      " [8 3 3 4 3]]\n",
      "\n",
      "ðŸ” FEATURE IMPORTANCE ANALYSIS:\n",
      "  Top 10 most important features:\n",
      "     1. Feature   16: 0.025530\n",
      "     2. Feature   18: 0.024834\n",
      "     3. Feature    1: 0.018524\n",
      "     4. Feature    4: 0.017288\n",
      "     5. Feature   62: 0.013960\n",
      "     6. Feature    0: 0.012692\n",
      "     7. Feature   55: 0.011821\n",
      "     8. Feature  161: 0.010799\n",
      "     9. Feature    5: 0.010325\n",
      "    10. Feature  190: 0.010011\n",
      "\n",
      "================================================================================\n",
      "ðŸ“ˆ STRONG IMPROVEMENT!\n",
      "F1 Score: 0.1645\n",
      "Improvement over baseline: +-0.3274 (+-66.6%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: EVALUATE ON VALIDATION SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[8/9] VALIDATION SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predict\n",
    "y_val_pred = classifier.predict(X_val_scaled)\n",
    "y_val_pred_proba = classifier.predict_proba(X_val_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
    "f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "precision = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_val, y_val_pred)\n",
    "f1_per_class = f1_score(y_val, y_val_pred, average=None, zero_division=0)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ FINAL RESULTS - HUBERT + RANDOM FOREST + BALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Classifier: Random Forest (n_estimators={classifier.n_estimators})\")\n",
    "print(f\"Balancing Strategy: {balancing_method}\")\n",
    "print(f\"Validation Accuracy:      {accuracy:.4f}\")\n",
    "print(f\"F1 Score (Macro):         {f1_macro:.4f} â­\")\n",
    "print(f\"F1 Score (Weighted):      {f1_weighted:.4f}\")\n",
    "print(f\"Precision (Macro):        {precision:.4f}\")\n",
    "print(f\"Recall (Macro):           {recall:.4f}\")\n",
    "print(f\"Cohen's Kappa:            {kappa:.4f}\")\n",
    "print(f\"CV F1 (Mean Â± Std):       {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nðŸ“Š PER-CLASS PERFORMANCE:\")\n",
    "available_classes = sorted(np.unique(y_val))\n",
    "for cls, f1_val in zip(available_classes, [f1_per_class[i] for i in available_classes]):\n",
    "    label = config.CLASS_NAMES[cls]\n",
    "    bar = 'â–ˆ' * int(f1_val * 40)\n",
    "    print(f\"  Class {cls} ({label:25s}): {f1_val:.4f} {bar}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"-\"*80)\n",
    "class_labels = [config.CLASS_NAMES[i] for i in available_classes]\n",
    "print(classification_report(y_val, y_val_pred, target_names=class_labels, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"\\nCONFUSION MATRIX:\")\n",
    "print(cm)\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(classifier, 'feature_importances_'):\n",
    "    print(f\"\\nðŸ” FEATURE IMPORTANCE ANALYSIS:\")\n",
    "    importances = classifier.feature_importances_\n",
    "    top_10_idx = np.argsort(importances)[-10:][::-1]\n",
    "    print(f\"  Top 10 most important features:\")\n",
    "    for i, idx in enumerate(top_10_idx):\n",
    "        print(f\"    {i+1:2d}. Feature {idx:4d}: {importances[idx]:.6f}\")\n",
    "\n",
    "# Achievement status\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "baseline_f1 = 0.4919\n",
    "target_f1 = 0.70\n",
    "improvement = f1_macro - baseline_f1\n",
    "\n",
    "if f1_macro >= target_f1:\n",
    "    print(f\"ðŸŽ‰ðŸŽ‰ðŸŽ‰ TARGET ACHIEVED! ðŸŽ‰ðŸŽ‰ðŸŽ‰\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f} >= {target_f1:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "elif f1_macro >= 0.65:\n",
    "    print(f\"ðŸš€ EXCELLENT PROGRESS!\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"Gap to target: -{target_f1 - f1_macro:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"ðŸ“ˆ STRONG IMPROVEMENT!\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27202c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[9/9] SAVING RESULTS\n",
      "================================================================================\n",
      "ðŸ’¾ Saving models...\n",
      "  âœ… Scaler saved\n",
      "  âœ… PCA saved\n",
      "  âœ… Classifier saved\n",
      "  âœ… Results saved: SAND_Project_Data\\optimized_hubert_rf_balanced\\results.json\n",
      "  âœ… Predictions saved: SAND_Project_Data\\optimized_hubert_rf_balanced\\validation_predictions.csv\n",
      "\n",
      "âœ… All results saved to: SAND_Project_Data\\optimized_hubert_rf_balanced\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: SAVE ALL MODELS AND RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[9/9] SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save models\n",
    "print(\"ðŸ’¾ Saving models...\")\n",
    "joblib.dump(scaler, os.path.join(config.OUTPUT_PATH, 'scaler.pkl'))\n",
    "print(f\"  âœ… Scaler saved\")\n",
    "\n",
    "if pca:\n",
    "    joblib.dump(pca, os.path.join(config.OUTPUT_PATH, 'pca.pkl'))\n",
    "    print(f\"  âœ… PCA saved\")\n",
    "\n",
    "joblib.dump(classifier, os.path.join(config.OUTPUT_PATH, 'classifier.pkl'))\n",
    "print(f\"  âœ… Classifier saved\")\n",
    "\n",
    "# Save results dictionary\n",
    "results = {\n",
    "    'model': 'Optimized HuBERT + Random Forest + Balanced Dataset',\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'strategy': 'HuBERT + Random Forest + Balanced Dataset',\n",
    "    'classifier_type': 'RandomForest',\n",
    "    'balancing_method': balancing_method,\n",
    "    'metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'kappa': float(kappa),\n",
    "        'cv_mean': float(cv_scores.mean()),\n",
    "        'cv_std': float(cv_scores.std()),\n",
    "    },\n",
    "    'f1_per_class': {int(i): float(f1_per_class[i]) for i in range(len(f1_per_class))},\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'configuration': {\n",
    "        'hubert_model': config.HUBERT_MODEL,\n",
    "        'multi_layer': config.USE_MULTI_LAYER,\n",
    "        'n_layers': len(config.LAYERS_TO_USE),\n",
    "        'layers': config.LAYERS_TO_USE,\n",
    "        'multi_pooling': config.USE_MULTI_POOLING,\n",
    "        'pooling_strategies': config.POOLING_STRATEGIES,\n",
    "        'statistical_features': config.USE_STATISTICAL_FEATURES,\n",
    "        'robust_scaling': config.USE_ROBUST_SCALING,\n",
    "        'pca': config.USE_PCA,\n",
    "        'pca_variance': config.PCA_VARIANCE if config.USE_PCA else None,\n",
    "        'balancing_strategy': config.BALANCING_STRATEGY,\n",
    "        'n_estimators': config.RF_CONFIG['n_estimators'],\n",
    "        'max_depth': config.RF_CONFIG['max_depth'],\n",
    "        'class_weight': config.RF_CONFIG['class_weight'],\n",
    "    },\n",
    "    'feature_dimensions': {\n",
    "        'original': int(X_train.shape[1]),\n",
    "        'after_pca': int(X_train_scaled.shape[1]),\n",
    "        'reduction_factor': float(X_train.shape[1] / X_train_scaled.shape[1]) if pca else 1.0\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'n_train': int(len(y_train)),\n",
    "        'n_train_resampled': int(len(y_train_resampled)),\n",
    "        'n_val': int(len(y_val)),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add feature importance if available\n",
    "if hasattr(classifier, 'feature_importances_'):\n",
    "    results['feature_importance'] = {\n",
    "        'mean_importance': float(classifier.feature_importances_.mean()),\n",
    "        'max_importance': float(classifier.feature_importances_.max()),\n",
    "        'min_importance': float(classifier.feature_importances_.min()),\n",
    "        'top_10_features': [int(idx) for idx in np.argsort(classifier.feature_importances_)[-10:][::-1]]\n",
    "    }\n",
    "\n",
    "results_path = os.path.join(config.OUTPUT_PATH, 'results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "print(f\"  âœ… Results saved: {results_path}\")\n",
    "\n",
    "# Save predictions\n",
    "val_predictions = pd.DataFrame({\n",
    "    'ID': ids_val,\n",
    "    'True_Class': y_val,\n",
    "    'Predicted_Class': y_val_pred,\n",
    "    'True_Label': [config.CLASS_NAMES[int(c)] for c in y_val],\n",
    "    'Predicted_Label': [config.CLASS_NAMES[int(c)] for c in y_val_pred],\n",
    "    'Correct': (y_val == y_val_pred).astype(int),\n",
    "    'Confidence': y_val_pred_proba.max(axis=1)\n",
    "})\n",
    "\n",
    "# Add probability columns\n",
    "for cls in range(len(config.CLASS_NAMES)):\n",
    "    val_predictions[f'Prob_Class_{cls}'] = y_val_pred_proba[:, cls]\n",
    "    val_predictions[f'Prob_{config.CLASS_NAMES[cls]}'] = y_val_pred_proba[:, cls]\n",
    "\n",
    "val_predictions = val_predictions.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "pred_path = os.path.join(config.OUTPUT_PATH, 'validation_predictions.csv')\n",
    "val_predictions.to_csv(pred_path, index=False)\n",
    "print(f\"  âœ… Predictions saved: {pred_path}\")\n",
    "\n",
    "print(f\"\\nâœ… All results saved to: {config.OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71cc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
