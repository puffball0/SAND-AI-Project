{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9fe4027",
   "metadata": {},
   "source": [
    "# Hubert Random Forest Classifier Unbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e239f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ HUBERT + RANDOM FOREST FOR SAND CHALLENGE\n",
      "Strategy: HuBERT + Random Forest + Unbalanced Dataset\n",
      "Target: F1 Score >= 0.70 | High Accuracy\n",
      "================================================================================\n",
      "âœ… Using device: cpu\n",
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ HUBERT + RANDOM FOREST FOR SAND CHALLENGE\")\n",
    "print(\"Strategy: HuBERT + Random Forest + Unbalanced Dataset\")\n",
    "print(\"Target: F1 Score >= 0.70 | High Accuracy\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, cohen_kappa_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4d07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“‹ CONFIGURATION - RANDOM FOREST + UNBALANCED DATASET\n",
      "================================================================================\n",
      "HuBERT Model: facebook/hubert-base-ls960\n",
      "Classifier: Random Forest (n_estimators=300)\n",
      "Dataset Strategy: UNBALANCED (preserving original distribution)\n",
      "Class Weight: balanced\n",
      "Multi-Layer Extraction: True\n",
      "Multi-Pooling: True\n",
      "Statistical Features: True\n",
      "PCA: True (variance=0.97)\n",
      "SMOTE: False (no balancing applied)\n",
      "Device: cpu\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Verifying paths...\n",
      "âœ… All paths verified!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: CONFIGURATION FOR RANDOM FOREST - UNBALANCED DATASET\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    \"\"\"Configuration for Random Forest with Unbalanced Dataset\"\"\"\n",
    "\n",
    "    # Paths - UPDATE THESE PATHS FOR YOUR LOCAL SYSTEM\n",
    "    BASE_PATH = 'SAND_Project_Data'  # Update this path\n",
    "    TRAINING_PATH = os.path.join(BASE_PATH, 'training')\n",
    "    EXCEL_PATH = os.path.join(BASE_PATH, 'sand_task_1.xlsx')\n",
    "    OUTPUT_PATH = os.path.join(BASE_PATH, 'hubert_rf_unbalanced')\n",
    "\n",
    "    # Sheet names\n",
    "    SHEET_TRAINING = 'Training Baseline - Task 1'\n",
    "    SHEET_VALIDATION = 'Validation Baseline - Task 1'\n",
    "\n",
    "    # Audio types\n",
    "    AUDIO_TYPES = ['phonationA', 'phonationE', 'phonationI', 'phonationO', 'phonationU',\n",
    "                   'rhythmKA', 'rhythmPA', 'rhythmTA']\n",
    "\n",
    "    # ========================================================================\n",
    "    # HUBERT CONFIGURATION\n",
    "    # ========================================================================\n",
    "    HUBERT_MODEL = 'facebook/hubert-base-ls960'\n",
    "    SAMPLING_RATE = 16000\n",
    "    MAX_AUDIO_LENGTH = 16000 * 15  # 15 seconds\n",
    "\n",
    "    # Multi-layer feature extraction\n",
    "    USE_MULTI_LAYER = True\n",
    "    LAYERS_TO_USE = [-4, -3, -2, -1]  # Last 4 layers\n",
    "\n",
    "    # Multi-pooling strategy\n",
    "    USE_MULTI_POOLING = True\n",
    "    POOLING_STRATEGIES = ['mean', 'std', 'max', 'min']  # 4 pooling methods\n",
    "\n",
    "    # Statistical features\n",
    "    USE_STATISTICAL_FEATURES = True\n",
    "\n",
    "    # Audio augmentation during loading\n",
    "    AUDIO_TRIM_DB = 15  # Lower threshold for dysarthric speech\n",
    "    AUDIO_NORMALIZE = True\n",
    "    MIN_AUDIO_LENGTH_SEC = 2\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREPROCESSING OPTIMIZATIONS\n",
    "    # ========================================================================\n",
    "    USE_ROBUST_SCALING = True  # Better for outliers\n",
    "\n",
    "    # PCA Configuration\n",
    "    USE_PCA = True\n",
    "    PCA_VARIANCE = 0.97  # Retain 97% variance\n",
    "\n",
    "    # ========================================================================\n",
    "    # DATA STRATEGY - UNBALANCED DATASET\n",
    "    # ========================================================================\n",
    "    USE_SMOTE = False  # No balancing for unbalanced dataset\n",
    "    USE_CLASS_WEIGHT = True  # Use class weights to handle imbalance\n",
    "\n",
    "    # ========================================================================\n",
    "    # RANDOM FOREST HYPERPARAMETERS FOR UNBALANCED DATA\n",
    "    # ========================================================================\n",
    "    RF_CONFIG = {\n",
    "        'n_estimators': 300,           # Fewer trees to prevent overfitting to majority class\n",
    "        'criterion': 'gini',           # Split quality measure\n",
    "        'max_depth': 20,               # Limit tree depth\n",
    "        'min_samples_split': 10,       # Higher to prevent overfitting\n",
    "        'min_samples_leaf': 4,         # Higher to prevent overfitting\n",
    "        'max_features': 'sqrt',        # Features to consider for best split\n",
    "        'bootstrap': True,             # Use bootstrap sampling\n",
    "        'class_weight': 'balanced',    # Balance classes using weights\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': -1,                  # Use all available cores\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Alternative config without class weights (for comparison)\n",
    "    RF_CONFIG_NO_WEIGHTS = {\n",
    "        'n_estimators': 300,\n",
    "        'criterion': 'gini',\n",
    "        'max_depth': 15,\n",
    "        'min_samples_split': 15,\n",
    "        'min_samples_leaf': 5,\n",
    "        'max_features': 'sqrt',\n",
    "        'bootstrap': True,\n",
    "        'class_weight': None,  # No class weights\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Cross-validation\n",
    "    N_FOLDS = 5\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "    # Class names\n",
    "    CLASS_NAMES = {\n",
    "        0: 'Severe Dysarthria',\n",
    "        1: 'Moderate Dysarthria',\n",
    "        2: 'Mild Dysarthria',\n",
    "        3: 'No Dysarthria (ALS)',\n",
    "        4: 'Healthy'\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ CONFIGURATION - RANDOM FOREST + UNBALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"HuBERT Model: {config.HUBERT_MODEL}\")\n",
    "print(f\"Classifier: Random Forest (n_estimators={config.RF_CONFIG['n_estimators']})\")\n",
    "print(f\"Dataset Strategy: UNBALANCED (preserving original distribution)\")\n",
    "print(f\"Class Weight: {config.RF_CONFIG['class_weight']}\")\n",
    "print(f\"Multi-Layer Extraction: {config.USE_MULTI_LAYER}\")\n",
    "print(f\"Multi-Pooling: {config.USE_MULTI_POOLING}\")\n",
    "print(f\"Statistical Features: {config.USE_STATISTICAL_FEATURES}\")\n",
    "print(f\"PCA: {config.USE_PCA} (variance={config.PCA_VARIANCE})\")\n",
    "print(f\"SMOTE: {config.USE_SMOTE} (no balancing applied)\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify paths\n",
    "print(f\"\\nðŸ” Verifying paths...\")\n",
    "assert os.path.exists(config.BASE_PATH), f\"Base path not found: {config.BASE_PATH}\"\n",
    "assert os.path.exists(config.TRAINING_PATH), f\"Training path not found: {config.TRAINING_PATH}\"\n",
    "assert os.path.exists(config.EXCEL_PATH), f\"Excel file not found: {config.EXCEL_PATH}\"\n",
    "print(\"âœ… All paths verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c99f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1/9] LOADING DATASETS\n",
      "================================================================================\n",
      "\n",
      "âœ… Loaded 219 samples from: 'Training Baseline - Task 1'\n",
      "ðŸ“Š Class distribution:\n",
      "  Class 0 (Severe Dysarthria        ):   4 ( 1.83%)\n",
      "  Class 1 (Moderate Dysarthria      ):  22 (10.05%)\n",
      "  Class 2 (Mild Dysarthria          ):  45 (20.55%)\n",
      "  Class 3 (No Dysarthria (ALS)      ):  62 (28.31%)\n",
      "  Class 4 (Healthy                  ):  86 (39.27%)\n",
      "\n",
      "âœ… Loaded 53 samples from: 'Validation Baseline - Task 1'\n",
      "ðŸ“Š Class distribution:\n",
      "  Class 0 (Severe Dysarthria        ):   2 ( 3.77%)\n",
      "  Class 1 (Moderate Dysarthria      ):   4 ( 7.55%)\n",
      "  Class 2 (Mild Dysarthria          ):  12 (22.64%)\n",
      "  Class 3 (No Dysarthria (ALS)      ):  14 (26.42%)\n",
      "  Class 4 (Healthy                  ):  21 (39.62%)\n",
      "\n",
      "ðŸ“ˆ Summary:\n",
      "  Training samples: 219\n",
      "  Validation samples: 53\n",
      "  Total audio files: 2176\n",
      "  Overlap check: âœ… No overlap\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: LOAD DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[1/9] LOADING DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_data(excel_path, sheet_name):\n",
    "    \"\"\"Load dataset from Excel\"\"\"\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    print(f\"\\nâœ… Loaded {len(df)} samples from: '{sheet_name}'\")\n",
    "\n",
    "    # Convert to 0-indexed classes\n",
    "    df['Class'] = df['Class'] - 1\n",
    "\n",
    "    # Display distribution\n",
    "    print(f\"ðŸ“Š Class distribution:\")\n",
    "    class_dist = df['Class'].value_counts().sort_index()\n",
    "    for cls, count in class_dist.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  Class {cls} ({config.CLASS_NAMES[cls]:25s}): {count:3d} ({pct:5.2f}%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load training and validation sets\n",
    "df_train = load_data(config.EXCEL_PATH, config.SHEET_TRAINING)\n",
    "df_val = load_data(config.EXCEL_PATH, config.SHEET_VALIDATION)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Summary:\")\n",
    "print(f\"  Training samples: {len(df_train)}\")\n",
    "print(f\"  Validation samples: {len(df_val)}\")\n",
    "print(f\"  Total audio files: {(len(df_train) + len(df_val)) * len(config.AUDIO_TYPES)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_ids = set(df_train['ID'].values)\n",
    "val_ids = set(df_val['ID'].values)\n",
    "overlap = train_ids.intersection(val_ids)\n",
    "print(f\"  Overlap check: {'âœ… No overlap' if len(overlap) == 0 else f'âš  {len(overlap)} overlapping IDs'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7ada46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2/9] LOADING HUBERT MODEL\n",
      "================================================================================\n",
      "Model: facebook/hubert-base-ls960\n",
      "â± This may take 1-2 minutes for first-time download...\n",
      "\n",
      "âœ… HuBERT loaded successfully!\n",
      "  Model size: 94.4M parameters\n",
      "  Hidden size: 768\n",
      "  Number of layers: 12\n",
      "  Sampling rate: 16000 Hz\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: LOAD HUBERT MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2/9] LOADING HUBERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {config.HUBERT_MODEL}\")\n",
    "print(\"â± This may take 1-2 minutes for first-time download...\")\n",
    "\n",
    "# Load feature extractor and model\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.HUBERT_MODEL)\n",
    "hubert_model = HubertModel.from_pretrained(config.HUBERT_MODEL)\n",
    "hubert_model = hubert_model.to(device)\n",
    "hubert_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"\\nâœ… HuBERT loaded successfully!\")\n",
    "print(f\"  Model size: {sum(p.numel() for p in hubert_model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"  Hidden size: {hubert_model.config.hidden_size}\")\n",
    "print(f\"  Number of layers: {hubert_model.config.num_hidden_layers}\")\n",
    "print(f\"  Sampling rate: {feature_extractor.sampling_rate} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1009bbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[3/9] DEFINING ENHANCED AUDIO PROCESSING\n",
      "================================================================================\n",
      "âœ… Audio processing functions defined!\n",
      "  Enhanced audio loading: âœ“\n",
      "  Statistical features (20 features): âœ“\n",
      "  Multi-layer HuBERT extraction: âœ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: ENHANCED AUDIO PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3/9] DEFINING ENHANCED AUDIO PROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_audio_enhanced(audio_path, target_sr=16000, max_length=None):\n",
    "    \"\"\"\n",
    "    Enhanced audio loading with advanced preprocessing\n",
    "\n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        target_sr: Target sampling rate\n",
    "        max_length: Maximum audio length in samples\n",
    "\n",
    "    Returns:\n",
    "        audio: Preprocessed audio waveform\n",
    "        sr: Sampling rate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path, sr=target_sr, mono=True)\n",
    "\n",
    "        # Advanced trimming with lower threshold for dysarthric speech\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=config.AUDIO_TRIM_DB)\n",
    "\n",
    "        # Normalize amplitude\n",
    "        if config.AUDIO_NORMALIZE:\n",
    "            audio = librosa.util.normalize(audio)\n",
    "\n",
    "        # Handle length constraints\n",
    "        if max_length and len(audio) > max_length:\n",
    "            audio = audio[:max_length]\n",
    "\n",
    "        # Ensure minimum length\n",
    "        min_length = target_sr * config.MIN_AUDIO_LENGTH_SEC\n",
    "        if len(audio) < min_length:\n",
    "            audio = np.pad(audio, (0, min_length - len(audio)), mode='constant')\n",
    "\n",
    "        return audio, sr\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error loading {audio_path}: {str(e)}\")\n",
    "        # Return silence as fallback\n",
    "        return np.zeros(target_sr * config.MIN_AUDIO_LENGTH_SEC), target_sr\n",
    "\n",
    "def extract_statistical_features(audio, sr=16000):\n",
    "    \"\"\"\n",
    "    Extract statistical and acoustic features from audio\n",
    "    These help capture dysarthria-specific patterns\n",
    "\n",
    "    Args:\n",
    "        audio: Audio waveform\n",
    "        sr: Sampling rate\n",
    "\n",
    "    Returns:\n",
    "        features: Array of statistical features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Time-domain statistics\n",
    "    features.extend([\n",
    "        np.mean(audio),           # Mean amplitude\n",
    "        np.std(audio),            # Standard deviation\n",
    "        skew(audio),              # Skewness\n",
    "        kurtosis(audio),          # Kurtosis\n",
    "        np.max(np.abs(audio)),    # Peak amplitude\n",
    "        np.median(audio),         # Median\n",
    "        np.percentile(audio, 25), # 25th percentile\n",
    "        np.percentile(audio, 75)  # 75th percentile\n",
    "    ])\n",
    "\n",
    "    # Zero crossing rate (voice quality indicator)\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio)[0]\n",
    "    features.extend([np.mean(zcr), np.std(zcr), np.max(zcr)])\n",
    "\n",
    "    # Energy features\n",
    "    energy = np.sum(audio**2) / len(audio)\n",
    "    features.append(energy)\n",
    "\n",
    "    # RMS energy\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    features.extend([np.mean(rms), np.std(rms)])\n",
    "\n",
    "    # Spectral features\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
    "    features.extend([np.mean(spectral_centroids), np.std(spectral_centroids)])\n",
    "\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
    "    features.extend([np.mean(spectral_rolloff), np.std(spectral_rolloff)])\n",
    "\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]\n",
    "    features.extend([np.mean(spectral_bandwidth), np.std(spectral_bandwidth)])\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_hubert_features_multilayer(audio, sampling_rate=16000):\n",
    "    \"\"\"\n",
    "    Extract HuBERT features with multi-layer and multi-pooling strategy\n",
    "    This is the KEY to high performance\n",
    "\n",
    "    Args:\n",
    "        audio: Audio waveform\n",
    "        sampling_rate: Sampling rate\n",
    "\n",
    "    Returns:\n",
    "        features: Concatenated feature vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess audio for HuBERT\n",
    "        inputs = feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Extract features with all hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = hubert_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        all_features = []\n",
    "\n",
    "        # Extract from multiple layers\n",
    "        if config.USE_MULTI_LAYER:\n",
    "            layers_to_process = config.LAYERS_TO_USE\n",
    "        else:\n",
    "            layers_to_process = [-1]  # Only last layer\n",
    "\n",
    "        for layer_idx in layers_to_process:\n",
    "            hidden_state = outputs.hidden_states[layer_idx]  # Shape: [batch, time, hidden]\n",
    "\n",
    "            # Apply multiple pooling strategies\n",
    "            if config.USE_MULTI_POOLING:\n",
    "                pooling_methods = config.POOLING_STRATEGIES\n",
    "            else:\n",
    "                pooling_methods = ['mean']  # Only mean pooling\n",
    "\n",
    "            for pool_strategy in pooling_methods:\n",
    "                if pool_strategy == 'mean':\n",
    "                    pooled = hidden_state.mean(dim=1)\n",
    "                elif pool_strategy == 'max':\n",
    "                    pooled = hidden_state.max(dim=1)[0]\n",
    "                elif pool_strategy == 'std':\n",
    "                    pooled = hidden_state.std(dim=1)\n",
    "                elif pool_strategy == 'min':\n",
    "                    pooled = hidden_state.min(dim=1)[0]\n",
    "                else:\n",
    "                    pooled = hidden_state.mean(dim=1)\n",
    "\n",
    "                all_features.append(pooled.cpu().numpy().squeeze())\n",
    "\n",
    "        # Concatenate all features\n",
    "        combined_features = np.concatenate(all_features)\n",
    "\n",
    "        return combined_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Feature extraction error: {str(e)}\")\n",
    "        # Return zero features as fallback\n",
    "        n_layers = len(config.LAYERS_TO_USE) if config.USE_MULTI_LAYER else 1\n",
    "        n_pooling = len(config.POOLING_STRATEGIES) if config.USE_MULTI_POOLING else 1\n",
    "        feature_dim = hubert_model.config.hidden_size * n_layers * n_pooling\n",
    "        return np.zeros(feature_dim)\n",
    "\n",
    "print(\"âœ… Audio processing functions defined!\")\n",
    "print(f\"  Enhanced audio loading: âœ“\")\n",
    "print(f\"  Statistical features ({20} features): âœ“\")\n",
    "print(f\"  Multi-layer HuBERT extraction: âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a29a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[4/9] EXTRACTING FEATURES\n",
      "================================================================================\n",
      "Configuration:\n",
      "  Layers: 4\n",
      "  Pooling strategies: 4\n",
      "  Statistical features: True\n",
      "  Feature multiplication factor: 16x\n",
      "\n",
      "ðŸŽµ Training 219 patients...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d796c47a304d4b85a29004ddc90d1a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽµ Validation 53 patients...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be55d77d2814a419d1f0c2840050e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Feature extraction complete!\n",
      "  Training shape: (219, 98464)\n",
      "  Validation shape: (53, 98464)\n",
      "  Feature dimension per patient: 98464\n",
      "  Data cleaned: âœ“\n",
      "  ðŸ’¾ Raw features saved: SAND_Project_Data\\hubert_rf_unbalanced\\features_raw.npz\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: EXTRACT FEATURES FROM ALL AUDIO FILES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4/9] EXTRACTING FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Layers: {len(config.LAYERS_TO_USE)}\")\n",
    "print(f\"  Pooling strategies: {len(config.POOLING_STRATEGIES)}\")\n",
    "print(f\"  Statistical features: {config.USE_STATISTICAL_FEATURES}\")\n",
    "print(f\"  Feature multiplication factor: {len(config.LAYERS_TO_USE) * len(config.POOLING_STRATEGIES)}x\")\n",
    "\n",
    "def extract_features_for_dataset(df, dataset_path, audio_types, desc=\"Processing\"):\n",
    "    \"\"\"\n",
    "    Extract enhanced features for entire dataset\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with patient IDs and labels\n",
    "        dataset_path: Path to audio files\n",
    "        audio_types: List of audio types\n",
    "        desc: Progress bar description\n",
    "\n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        ids: Patient IDs\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    ids_list = []\n",
    "\n",
    "    print(f\"\\nðŸŽµ {desc} {len(df)} patients...\")\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        patient_id = row['ID']\n",
    "        patient_class = row['Class']\n",
    "\n",
    "        patient_features = []\n",
    "\n",
    "        # Process each audio type\n",
    "        for audio_type in audio_types:\n",
    "            audio_file = f\"{patient_id}_{audio_type}.wav\"\n",
    "            audio_path = os.path.join(dataset_path, audio_type, audio_file)\n",
    "\n",
    "            if os.path.exists(audio_path):\n",
    "                # Load audio with enhancements\n",
    "                audio, sr = load_audio_enhanced(\n",
    "                    audio_path,\n",
    "                    target_sr=config.SAMPLING_RATE,\n",
    "                    max_length=config.MAX_AUDIO_LENGTH\n",
    "                )\n",
    "\n",
    "                # Extract HuBERT features\n",
    "                hubert_features = extract_hubert_features_multilayer(audio, sr)\n",
    "\n",
    "                # Extract statistical features if enabled\n",
    "                if config.USE_STATISTICAL_FEATURES:\n",
    "                    stat_features = extract_statistical_features(audio, sr)\n",
    "                    combined_features = np.concatenate([hubert_features, stat_features])\n",
    "                else:\n",
    "                    combined_features = hubert_features\n",
    "\n",
    "                patient_features.append(combined_features)\n",
    "            else:\n",
    "                # Zero features for missing files\n",
    "                n_layers = len(config.LAYERS_TO_USE) if config.USE_MULTI_LAYER else 1\n",
    "                n_pooling = len(config.POOLING_STRATEGIES) if config.USE_MULTI_POOLING else 1\n",
    "                feature_dim = hubert_model.config.hidden_size * n_layers * n_pooling\n",
    "\n",
    "                if config.USE_STATISTICAL_FEATURES:\n",
    "                    feature_dim += 20  # Statistical features count\n",
    "\n",
    "                patient_features.append(np.zeros(feature_dim))\n",
    "\n",
    "        # Concatenate features from all audio types\n",
    "        combined = np.concatenate(patient_features)\n",
    "        features_list.append(combined)\n",
    "        labels_list.append(patient_class)\n",
    "        ids_list.append(patient_id)\n",
    "\n",
    "    X = np.array(features_list)\n",
    "    y = np.array(labels_list)\n",
    "    ids = np.array(ids_list)\n",
    "\n",
    "    return X, y, ids\n",
    "\n",
    "# Extract training features\n",
    "X_train, y_train, ids_train = extract_features_for_dataset(\n",
    "    df_train,\n",
    "    config.TRAINING_PATH,\n",
    "    config.AUDIO_TYPES,\n",
    "    desc=\"Training\"\n",
    ")\n",
    "\n",
    "# Extract validation features\n",
    "X_val, y_val, ids_val = extract_features_for_dataset(\n",
    "    df_val,\n",
    "    config.TRAINING_PATH,\n",
    "    config.AUDIO_TYPES,\n",
    "    desc=\"Validation\"\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Feature extraction complete!\")\n",
    "print(f\"  Training shape: {X_train.shape}\")\n",
    "print(f\"  Validation shape: {X_val.shape}\")\n",
    "print(f\"  Feature dimension per patient: {X_train.shape[1]}\")\n",
    "\n",
    "# Clean data (handle any NaN/Inf)\n",
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"  Data cleaned: âœ“\")\n",
    "\n",
    "# Save raw features\n",
    "features_path = os.path.join(config.OUTPUT_PATH, 'features_raw.npz')\n",
    "np.savez(\n",
    "    features_path,\n",
    "    X_train=X_train, y_train=y_train, ids_train=ids_train,\n",
    "    X_val=X_val, y_val=y_val, ids_val=ids_val\n",
    ")\n",
    "print(f\"  ðŸ’¾ Raw features saved: {features_path}\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9beed741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[5/9] PREPROCESSING FOR UNBALANCED DATASET\n",
      "================================================================================\n",
      "ðŸ“Š Applying RobustScaler (better for outliers)...\n",
      "  âœ… Scaling complete\n",
      "     Train: mean=0.022045, std=0.781847\n",
      "     Val:   mean=0.033796, std=0.821249\n",
      "\n",
      "ðŸ“‰ Applying PCA (variance threshold=0.97)...\n",
      "  âœ… PCA complete\n",
      "     Original dimensions: 98464\n",
      "     Reduced dimensions: 199\n",
      "     Explained variance: 97.14%\n",
      "     Dimensionality reduction: 494.8x\n",
      "\n",
      "ðŸ“Š PRESERVING UNBALANCED DATASET DISTRIBUTION\n",
      "  Original training class distribution:\n",
      "    Class 0 (Severe Dysarthria        ):   4 (  1.8%)\n",
      "    Class 1 (Moderate Dysarthria      ):  22 ( 10.0%)\n",
      "    Class 2 (Mild Dysarthria          ):  45 ( 20.5%)\n",
      "    Class 3 (No Dysarthria (ALS)      ):  62 ( 28.3%)\n",
      "    Class 4 (Healthy                  ):  86 ( 39.3%)\n",
      "\n",
      "  âœ… Using original unbalanced dataset\n",
      "     Training samples: 219\n",
      "     Class imbalance preserved\n",
      "     Majority class: Class 4 (86 samples)\n",
      "     Minority class: Class 0 (4 samples)\n",
      "     Imbalance ratio: 21.5x\n",
      "\n",
      "âœ… Preprocessing complete! Using unbalanced dataset.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: PREPROCESSING PIPELINE - UNBALANCED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5/9] PREPROCESSING FOR UNBALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Scaling\n",
    "if config.USE_ROBUST_SCALING:\n",
    "    print(\"ðŸ“Š Applying RobustScaler (better for outliers)...\")\n",
    "    scaler = RobustScaler()\n",
    "else:\n",
    "    print(\"ðŸ“Š Applying StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"  âœ… Scaling complete\")\n",
    "print(f\"     Train: mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")\n",
    "print(f\"     Val:   mean={X_val_scaled.mean():.6f}, std={X_val_scaled.std():.6f}\")\n",
    "\n",
    "# Step 2: PCA (Dimensionality Reduction)\n",
    "if config.USE_PCA:\n",
    "    print(f\"\\nðŸ“‰ Applying PCA (variance threshold={config.PCA_VARIANCE})...\")\n",
    "    pca = PCA(n_components=config.PCA_VARIANCE, random_state=config.RANDOM_SEED)\n",
    "    X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "    X_val_scaled = pca.transform(X_val_scaled)\n",
    "\n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"  âœ… PCA complete\")\n",
    "    print(f\"     Original dimensions: {X_train.shape[1]}\")\n",
    "    print(f\"     Reduced dimensions: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"     Explained variance: {explained_var*100:.2f}%\")\n",
    "    print(f\"     Dimensionality reduction: {X_train.shape[1]/X_train_scaled.shape[1]:.1f}x\")\n",
    "else:\n",
    "    pca = None\n",
    "    print(\"  â„¹ PCA disabled\")\n",
    "\n",
    "# Step 3: No Data Balancing - Preserve Original Distribution\n",
    "print(f\"\\nðŸ“Š PRESERVING UNBALANCED DATASET DISTRIBUTION\")\n",
    "print(f\"  Original training class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "total_samples = len(y_train)\n",
    "for cls, count in zip(unique, counts):\n",
    "    pct = (count / total_samples) * 100\n",
    "    print(f\"    Class {cls} ({config.CLASS_NAMES[cls]:25s}): {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "# No resampling - use original data\n",
    "X_train_resampled = X_train_scaled\n",
    "y_train_resampled = y_train\n",
    "\n",
    "print(f\"\\n  âœ… Using original unbalanced dataset\")\n",
    "print(f\"     Training samples: {len(y_train_resampled)}\")\n",
    "print(f\"     Class imbalance preserved\")\n",
    "print(f\"     Majority class: Class {np.argmax(counts)} ({np.max(counts)} samples)\")\n",
    "print(f\"     Minority class: Class {np.argmin(counts)} ({np.min(counts)} samples)\")\n",
    "print(f\"     Imbalance ratio: {np.max(counts)/np.min(counts):.1f}x\")\n",
    "\n",
    "print(\"\\nâœ… Preprocessing complete! Using unbalanced dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15faa9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[6/9] TRAINING RANDOM FOREST ON UNBALANCED DATA\n",
      "================================================================================\n",
      "Configuration:\n",
      "  n_estimators: 300\n",
      "  criterion: gini\n",
      "  max_depth: 20\n",
      "  min_samples_split: 10\n",
      "  min_samples_leaf: 4\n",
      "  max_features: sqrt\n",
      "  bootstrap: True\n",
      "  class_weight: balanced\n",
      "  random_state: 42\n",
      "\n",
      "ðŸŒ² Training Random Forest on 219 unbalanced samples...\n",
      "   Dataset strategy: UNBALANCED\n",
      "   Class weight: balanced\n",
      "\n",
      "  Class distribution in training:\n",
      "    Class 0:   4 samples, weight: 10.95\n",
      "    Class 1:  22 samples, weight: 1.99\n",
      "    Class 2:  45 samples, weight: 0.97\n",
      "    Class 3:  62 samples, weight: 0.71\n",
      "    Class 4:  86 samples, weight: 0.51\n",
      "âœ… Random Forest training complete!\n",
      "  Number of trees: 300\n",
      "  Number of features: 199\n",
      "  Dataset imbalance handled via: class weights\n",
      "  Feature importance range: 0.001209 - 0.035200\n",
      "  Top 5 most important features:\n",
      "     1. Feature    0: 0.035200\n",
      "     2. Feature    1: 0.033026\n",
      "     3. Feature    4: 0.029581\n",
      "     4. Feature   16: 0.024145\n",
      "     5. Feature   18: 0.022651\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: TRAIN RANDOM FOREST ON UNBALANCED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6/9] TRAINING RANDOM FOREST ON UNBALANCED DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration:\")\n",
    "for key, value in config.RF_CONFIG.items():\n",
    "    if key not in ['n_jobs', 'verbose']:  # Skip technical parameters\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸŒ² Training Random Forest on {len(y_train_resampled)} unbalanced samples...\")\n",
    "print(f\"   Dataset strategy: UNBALANCED\")\n",
    "print(f\"   Class weight: {config.RF_CONFIG['class_weight']}\")\n",
    "\n",
    "# Calculate and display class distribution for reference\n",
    "class_counts = np.bincount(y_train_resampled)\n",
    "class_weights = {}\n",
    "print(f\"\\n  Class distribution in training:\")\n",
    "for cls in range(len(config.CLASS_NAMES)):\n",
    "    if cls < len(class_counts):\n",
    "        count = class_counts[cls]\n",
    "        weight = total_samples / (len(config.CLASS_NAMES) * count) if config.USE_CLASS_WEIGHT else 1.0\n",
    "        class_weights[cls] = weight\n",
    "        print(f\"    Class {cls}: {count:3d} samples, weight: {weight:.2f}\")\n",
    "\n",
    "# Use class weights if specified\n",
    "if config.USE_CLASS_WEIGHT:\n",
    "    rf_config = config.RF_CONFIG.copy()\n",
    "else:\n",
    "    rf_config = config.RF_CONFIG_NO_WEIGHTS.copy()\n",
    "    print(f\"  âš  Training without class weights\")\n",
    "\n",
    "# Initialize and train classifier\n",
    "classifier = RandomForestClassifier(**rf_config)\n",
    "classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(f\"âœ… Random Forest training complete!\")\n",
    "print(f\"  Number of trees: {classifier.n_estimators}\")\n",
    "print(f\"  Number of features: {classifier.n_features_in_}\")\n",
    "print(f\"  Dataset imbalance handled via: {'class weights' if config.USE_CLASS_WEIGHT else 'no balancing'}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if hasattr(classifier, 'feature_importances_'):\n",
    "    importances = classifier.feature_importances_\n",
    "    print(f\"  Feature importance range: {importances.min():.6f} - {importances.max():.6f}\")\n",
    "    \n",
    "    # Top 5 features\n",
    "    top_5_idx = np.argsort(importances)[-5:][::-1]\n",
    "    print(f\"  Top 5 most important features:\")\n",
    "    for i, idx in enumerate(top_5_idx):\n",
    "        print(f\"    {i+1:2d}. Feature {idx:4d}: {importances[idx]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b9c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[7/9] CROSS-VALIDATION ON UNBALANCED DATA\n",
      "================================================================================\n",
      "Performing 5-fold stratified cross-validation...\n",
      "â± This may take a few minutes for Random Forest...\n",
      "\n",
      "âœ… Cross-validation complete!\n",
      "  Fold scores: ['0.2091', '0.2500', '0.2049', '0.2301', '0.1823']\n",
      "  Mean CV F1: 0.2153\n",
      "  Std CV F1:  0.0231\n",
      "  Min CV F1:  0.1823\n",
      "  Max CV F1:  0.2500\n",
      "  Mean CV Accuracy: 0.4246\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: CROSS-VALIDATION ON UNBALANCED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7/9] CROSS-VALIDATION ON UNBALANCED DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Performing {config.N_FOLDS}-fold stratified cross-validation...\")\n",
    "print(\"â± This may take a few minutes for Random Forest...\")\n",
    "\n",
    "# Cross-validation on ORIGINAL unbalanced training set\n",
    "cv_classifier = RandomForestClassifier(**config.RF_CONFIG)\n",
    "cv_scores = cross_val_score(\n",
    "    cv_classifier,\n",
    "    X_train_scaled,\n",
    "    y_train,  # Original unbalanced labels\n",
    "    cv=config.N_FOLDS,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Cross-validation complete!\")\n",
    "print(f\"  Fold scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "print(f\"  Mean CV F1: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std CV F1:  {cv_scores.std():.4f}\")\n",
    "print(f\"  Min CV F1:  {cv_scores.min():.4f}\")\n",
    "print(f\"  Max CV F1:  {cv_scores.max():.4f}\")\n",
    "\n",
    "# Additional metrics for unbalanced data\n",
    "cv_accuracy = cross_val_score(\n",
    "    cv_classifier,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=config.N_FOLDS,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"  Mean CV Accuracy: {cv_accuracy.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "348e27da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[8/9] VALIDATION SET EVALUATION - UNBALANCED DATASET\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ FINAL RESULTS - HUBERT + RANDOM FOREST + UNBALANCED DATASET\n",
      "================================================================================\n",
      "Classifier: Random Forest (n_estimators=300)\n",
      "Dataset Strategy: UNBALANCED\n",
      "Class Weight: balanced\n",
      "Validation Accuracy:      0.3962\n",
      "F1 Score (Macro):         0.1698 â­\n",
      "F1 Score (Weighted):      0.2448\n",
      "Precision (Macro):        0.1467\n",
      "Recall (Macro):           0.2405\n",
      "Cohen's Kappa:            0.0292\n",
      "CV F1 (Mean Â± Std):       0.2153 Â± 0.0231\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PER-CLASS PERFORMANCE (UNBALANCED DATASET):\n",
      "  ðŸ”´ Class 0 (Severe Dysarthria        ): 0.0000  (samples: 2)\n",
      "  ðŸ”´ Class 1 (Moderate Dysarthria      ): 0.2857 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (samples: 4)\n",
      "  ðŸŸ¡ Class 2 (Mild Dysarthria          ): 0.0000  (samples: 12)\n",
      "  ðŸŸ¡ Class 3 (No Dysarthria (ALS)      ): 0.0000  (samples: 14)\n",
      "  ðŸŸ¢ Class 4 (Healthy                  ): 0.5634 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (samples: 21)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CLASSIFICATION REPORT:\n",
      "--------------------------------------------------------------------------------\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  Severe Dysarthria       0.00      0.00      0.00         2\n",
      "Moderate Dysarthria       0.33      0.25      0.29         4\n",
      "    Mild Dysarthria       0.00      0.00      0.00        12\n",
      "No Dysarthria (ALS)       0.00      0.00      0.00        14\n",
      "            Healthy       0.40      0.95      0.56        21\n",
      "\n",
      "           accuracy                           0.40        53\n",
      "          macro avg       0.15      0.24      0.17        53\n",
      "       weighted avg       0.18      0.40      0.24        53\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "[[ 0  1  0  0  1]\n",
      " [ 0  1  0  0  3]\n",
      " [ 0  0  0  0 12]\n",
      " [ 0  0  0  0 14]\n",
      " [ 0  1  0  0 20]]\n",
      "\n",
      "ðŸ” MINORITY CLASS ANALYSIS:\n",
      "  Minority Class 0 F1: 0.0000\n",
      "\n",
      "ðŸ” FEATURE IMPORTANCE ANALYSIS:\n",
      "  Top 10 most important features:\n",
      "     1. Feature    0: 0.035200\n",
      "     2. Feature    1: 0.033026\n",
      "     3. Feature    4: 0.029581\n",
      "     4. Feature   16: 0.024145\n",
      "     5. Feature   18: 0.022651\n",
      "     6. Feature    7: 0.018865\n",
      "     7. Feature    2: 0.015576\n",
      "     8. Feature   62: 0.015003\n",
      "     9. Feature   51: 0.012606\n",
      "    10. Feature   57: 0.011444\n",
      "\n",
      "================================================================================\n",
      "ðŸ“ˆ GOOD PERFORMANCE WITH UNBALANCED DATA!\n",
      "F1 Score: 0.1698\n",
      "Improvement over baseline: +-0.3221 (+-65.5%)\n",
      "ðŸ’¡ Consider using class weights or balanced RF for better minority class performance\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: EVALUATE ON VALIDATION SET - UNBALANCED FOCUS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[8/9] VALIDATION SET EVALUATION - UNBALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Predict\n",
    "y_val_pred = classifier.predict(X_val_scaled)\n",
    "y_val_pred_proba = classifier.predict_proba(X_val_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
    "f1_weighted = f1_score(y_val, y_val_pred, average='weighted')\n",
    "precision = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_val, y_val_pred)\n",
    "f1_per_class = f1_score(y_val, y_val_pred, average=None, zero_division=0)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ FINAL RESULTS - HUBERT + RANDOM FOREST + UNBALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Classifier: Random Forest (n_estimators={classifier.n_estimators})\")\n",
    "print(f\"Dataset Strategy: UNBALANCED\")\n",
    "print(f\"Class Weight: {config.RF_CONFIG['class_weight']}\")\n",
    "print(f\"Validation Accuracy:      {accuracy:.4f}\")\n",
    "print(f\"F1 Score (Macro):         {f1_macro:.4f} â­\")\n",
    "print(f\"F1 Score (Weighted):      {f1_weighted:.4f}\")\n",
    "print(f\"Precision (Macro):        {precision:.4f}\")\n",
    "print(f\"Recall (Macro):           {recall:.4f}\")\n",
    "print(f\"Cohen's Kappa:            {kappa:.4f}\")\n",
    "print(f\"CV F1 (Mean Â± Std):       {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Per-class metrics with emphasis on minority classes\n",
    "print(\"\\nðŸ“Š PER-CLASS PERFORMANCE (UNBALANCED DATASET):\")\n",
    "available_classes = sorted(np.unique(y_val))\n",
    "val_class_counts = np.bincount(y_val)\n",
    "\n",
    "for cls, f1_val in zip(available_classes, [f1_per_class[i] for i in available_classes]):\n",
    "    label = config.CLASS_NAMES[cls]\n",
    "    count = val_class_counts[cls] if cls < len(val_class_counts) else 0\n",
    "    bar = 'â–ˆ' * int(f1_val * 40)\n",
    "    status = 'ðŸ”´' if count < 10 else 'ðŸŸ¡' if count < 20 else 'ðŸŸ¢'  # Color code by sample size\n",
    "    print(f\"  {status} Class {cls} ({label:25s}): {f1_val:.4f} {bar} (samples: {count})\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"-\"*80)\n",
    "class_labels = [config.CLASS_NAMES[i] for i in available_classes]\n",
    "print(classification_report(y_val, y_val_pred, target_names=class_labels, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"\\nCONFUSION MATRIX:\")\n",
    "print(cm)\n",
    "\n",
    "# Minority class performance analysis\n",
    "print(f\"\\nðŸ” MINORITY CLASS ANALYSIS:\")\n",
    "minority_classes = [cls for cls in available_classes if val_class_counts[cls] == min(val_class_counts)]\n",
    "for cls in minority_classes:\n",
    "    if cls < len(f1_per_class):\n",
    "        print(f\"  Minority Class {cls} F1: {f1_per_class[cls]:.4f}\")\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(classifier, 'feature_importances_'):\n",
    "    print(f\"\\nðŸ” FEATURE IMPORTANCE ANALYSIS:\")\n",
    "    importances = classifier.feature_importances_\n",
    "    top_10_idx = np.argsort(importances)[-10:][::-1]\n",
    "    print(f\"  Top 10 most important features:\")\n",
    "    for i, idx in enumerate(top_10_idx):\n",
    "        print(f\"    {i+1:2d}. Feature {idx:4d}: {importances[idx]:.6f}\")\n",
    "\n",
    "# Achievement status with unbalanced dataset context\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "baseline_f1 = 0.4919\n",
    "target_f1 = 0.70\n",
    "improvement = f1_macro - baseline_f1\n",
    "\n",
    "if f1_macro >= target_f1:\n",
    "    print(f\"ðŸŽ‰ðŸŽ‰ðŸŽ‰ TARGET ACHIEVED WITH UNBALANCED DATA! ðŸŽ‰ðŸŽ‰ðŸŽ‰\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f} >= {target_f1:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "elif f1_macro >= 0.65:\n",
    "    print(f\"ðŸš€ EXCELLENT PROGRESS WITH UNBALANCED DATA!\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"Gap to target: -{target_f1 - f1_macro:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"ðŸ“ˆ GOOD PERFORMANCE WITH UNBALANCED DATA!\")\n",
    "    print(f\"F1 Score: {f1_macro:.4f}\")\n",
    "    print(f\"Improvement over baseline: +{improvement:.4f} (+{improvement/baseline_f1*100:.1f}%)\")\n",
    "    print(f\"ðŸ’¡ Consider using class weights or balanced RF for better minority class performance\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba175b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[9/9] SAVING RESULTS - UNBALANCED DATASET\n",
      "================================================================================\n",
      "ðŸ’¾ Saving models...\n",
      "  âœ… Scaler saved\n",
      "  âœ… PCA saved\n",
      "  âœ… Classifier saved\n",
      "  âœ… Results saved: SAND_Project_Data\\hubert_rf_unbalanced\\results.json\n",
      "  âœ… Predictions saved: SAND_Project_Data\\hubert_rf_unbalanced\\validation_predictions.csv\n",
      "\n",
      "âœ… All results saved to: SAND_Project_Data\\hubert_rf_unbalanced\n",
      "\n",
      "ðŸ“Š UNBALANCED DATASET STRATEGY SUMMARY:\n",
      "  â€¢ Preserved original class distribution\n",
      "  â€¢ Used class weights: True\n",
      "  â€¢ Random Forest with 300 trees\n",
      "  â€¢ Final F1 Score: 0.1698\n",
      "  â€¢ Dataset imbalance handled via internal class weighting\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: SAVE ALL MODELS AND RESULTS - UNBALANCED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[9/9] SAVING RESULTS - UNBALANCED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save models\n",
    "print(\"ðŸ’¾ Saving models...\")\n",
    "joblib.dump(scaler, os.path.join(config.OUTPUT_PATH, 'scaler.pkl'))\n",
    "print(f\"  âœ… Scaler saved\")\n",
    "\n",
    "if pca:\n",
    "    joblib.dump(pca, os.path.join(config.OUTPUT_PATH, 'pca.pkl'))\n",
    "    print(f\"  âœ… PCA saved\")\n",
    "\n",
    "joblib.dump(classifier, os.path.join(config.OUTPUT_PATH, 'classifier.pkl'))\n",
    "print(f\"  âœ… Classifier saved\")\n",
    "\n",
    "# Save results dictionary with unbalanced dataset focus\n",
    "results = {\n",
    "    'model': 'HuBERT + Random Forest + Unbalanced Dataset',\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'strategy': 'HuBERT + Random Forest + Unbalanced Dataset',\n",
    "    'classifier_type': 'RandomForest',\n",
    "    'dataset_strategy': 'unbalanced',\n",
    "    'class_weight_used': config.USE_CLASS_WEIGHT,\n",
    "    'metrics': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'f1_weighted': float(f1_weighted),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'kappa': float(kappa),\n",
    "        'cv_mean': float(cv_scores.mean()),\n",
    "        'cv_std': float(cv_scores.std()),\n",
    "    },\n",
    "    'f1_per_class': {int(i): float(f1_per_class[i]) for i in range(len(f1_per_class))},\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'dataset_distribution': {\n",
    "        'training_samples_per_class': {int(cls): int(count) for cls, count in enumerate(np.bincount(y_train))},\n",
    "        'validation_samples_per_class': {int(cls): int(count) for cls, count in enumerate(np.bincount(y_val))},\n",
    "        'imbalance_ratio_training': float(np.max(np.bincount(y_train)) / np.min(np.bincount(y_train))),\n",
    "        'imbalance_ratio_validation': float(np.max(np.bincount(y_val)) / np.min(np.bincount(y_val)))\n",
    "    },\n",
    "    'configuration': {\n",
    "        'hubert_model': config.HUBERT_MODEL,\n",
    "        'multi_layer': config.USE_MULTI_LAYER,\n",
    "        'n_layers': len(config.LAYERS_TO_USE),\n",
    "        'layers': config.LAYERS_TO_USE,\n",
    "        'multi_pooling': config.USE_MULTI_POOLING,\n",
    "        'pooling_strategies': config.POOLING_STRATEGIES,\n",
    "        'statistical_features': config.USE_STATISTICAL_FEATURES,\n",
    "        'robust_scaling': config.USE_ROBUST_SCALING,\n",
    "        'pca': config.USE_PCA,\n",
    "        'pca_variance': config.PCA_VARIANCE if config.USE_PCA else None,\n",
    "        'balancing_strategy': 'unbalanced',\n",
    "        'class_weight': config.RF_CONFIG['class_weight'],\n",
    "        'n_estimators': config.RF_CONFIG['n_estimators'],\n",
    "        'max_depth': config.RF_CONFIG['max_depth'],\n",
    "    },\n",
    "    'feature_dimensions': {\n",
    "        'original': int(X_train.shape[1]),\n",
    "        'after_pca': int(X_train_scaled.shape[1]),\n",
    "        'reduction_factor': float(X_train.shape[1] / X_train_scaled.shape[1]) if pca else 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add feature importance if available\n",
    "if hasattr(classifier, 'feature_importances_'):\n",
    "    results['feature_importance'] = {\n",
    "        'mean_importance': float(classifier.feature_importances_.mean()),\n",
    "        'max_importance': float(classifier.feature_importances_.max()),\n",
    "        'min_importance': float(classifier.feature_importances_.min()),\n",
    "        'top_10_features': [int(idx) for idx in np.argsort(classifier.feature_importances_)[-10:][::-1]]\n",
    "    }\n",
    "\n",
    "results_path = os.path.join(config.OUTPUT_PATH, 'results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "print(f\"  âœ… Results saved: {results_path}\")\n",
    "\n",
    "# Save predictions\n",
    "val_predictions = pd.DataFrame({\n",
    "    'ID': ids_val,\n",
    "    'True_Class': y_val,\n",
    "    'Predicted_Class': y_val_pred,\n",
    "    'True_Label': [config.CLASS_NAMES[int(c)] for c in y_val],\n",
    "    'Predicted_Label': [config.CLASS_NAMES[int(c)] for c in y_val_pred],\n",
    "    'Correct': (y_val == y_val_pred).astype(int),\n",
    "    'Confidence': y_val_pred_proba.max(axis=1)\n",
    "})\n",
    "\n",
    "# Add probability columns\n",
    "for cls in range(len(config.CLASS_NAMES)):\n",
    "    val_predictions[f'Prob_Class_{cls}'] = y_val_pred_proba[:, cls]\n",
    "    val_predictions[f'Prob_{config.CLASS_NAMES[cls]}'] = y_val_pred_proba[:, cls]\n",
    "\n",
    "val_predictions = val_predictions.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "pred_path = os.path.join(config.OUTPUT_PATH, 'validation_predictions.csv')\n",
    "val_predictions.to_csv(pred_path, index=False)\n",
    "print(f\"  âœ… Predictions saved: {pred_path}\")\n",
    "\n",
    "print(f\"\\nâœ… All results saved to: {config.OUTPUT_PATH}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nðŸ“Š UNBALANCED DATASET STRATEGY SUMMARY:\")\n",
    "print(f\"  â€¢ Preserved original class distribution\")\n",
    "print(f\"  â€¢ Used class weights: {config.USE_CLASS_WEIGHT}\")\n",
    "print(f\"  â€¢ Random Forest with {classifier.n_estimators} trees\")\n",
    "print(f\"  â€¢ Final F1 Score: {f1_macro:.4f}\")\n",
    "print(f\"  â€¢ Dataset imbalance handled via internal class weighting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
